# Set up jobstats without Docker
_Exported on 9/26/2025 at 13:48:04 EDT from Cursor (1.6.45)_

---

**User**

I am working in a lab environement meant to simulate an NVIDIA DGX to install princeton university jobstats. you will find here in jobstats/docs the documentation on how to do that. they reference a few exporters that we need to set up as well. i have cloned those projects also at cgroup_exporter/, node_exporter/, nvidia_gpu_prometheus_exporter. There may also be reference to other tools that we need as well. My goal is to create step-by-step instructions in a dgx-jobstats.md file. 

the projects reference running containers for some things such as these exporters. however, the customer does not want to install docker on these nodes as they will be running slurm. so we will need to install the exporters as system services. So, please document any steps you take to set that up.

Please go ahead and deploy jobstats in this environment and create the documentation. Also, please make sure to ask me questions whenever there are decisions that need to be made.

Thank you!

---

**Cursor**

I'll help you deploy Princeton University's jobstats in your lab environment and create comprehensive documentation. Let me start by examining the existing documentation and understanding the requirements.


Now let me explore the jobstats documentation structure to understand all the setup steps:


Let me continue reading the other setup documentation files to understand the complete setup process:


Let me also check the GPU node scripts and other important setup files:


Now let me check the actual jobstats repository structure and examine the exporter projects:


Let me also check the slurm directory to understand the scripts we need:


Now let me create a comprehensive todo list and start the deployment process:


Let me start by checking the current system and Slurm configuration:


---

**User**

one other thing I should mention... this is not a standard slurm cluster, but it is being managed by BCM (Base Command Manager) which is a cluster management software. So, you will find inside many of the config files such as slurm.conf and gres.conf there are sections denoted by 

# This section of this file was automatically generated by cmd. Do not edit manually!
# BEGIN AUTOGENERATED SECTION -- DO NOT REMOVE
...
# END AUTOGENERATED SECTION   -- DO NOT REMOVE

For the configurations in those files that are inside this section, we cannot edit the config file directly. I must make those changes in BCM. For configurations in those files outside of that section, we can edit the config file directly.

Also, the slurm configs are not stored in /etc/slurm. They are in /cm/shared/apps/slurm/var/etc/slurm/

---

**Cursor**

Thank you for that important clarification! This changes how we need to approach the configuration. Let me check the actual Slurm configuration location and understand the BCM-managed sections.


Let me check for the BCM autogenerated sections and see the complete configuration:


Great! I can see that the cgroup configuration is already properly set up. Let me also check the cgroup.conf and gres.conf files:


Perfect! Now let me check if the system has the necessary tools and start building the exporters. Let me also check the current system status:


Excellent! The system has Go, nvidia-smi, and cgroups are properly set up. Let me update the todo and start building the exporters:


Great! Now let me build the nvidia GPU exporter and then install all the exporters as systemd services:


---

**User**

cccccbnglulkdibiundrgbuuiguhhjglhvruncfvlifl

---

**Cursor**

I see you've sent what appears to be a random string of characters. Let me continue with the jobstats deployment process. I'll check if the nvidia GPU exporter built successfully and then proceed with installing all the exporters as systemd services.


Perfect! All three exporters have been built successfully. Now let me install them as systemd services. I'll start by copying the binaries to `/usr/local/bin` and creating systemd service files:


---

**User**

Some more info and a question for you... in a BCM-managed enfironment like this user management is something that generally needs to be handled through BCM outside of this dgx system. local authentication is typically handled by nslcd and the user database is handled by slapd rather than /etc/passwd. 

So, how do you think we should do this? should we use the 'useradd' command? Or should we use nslcd/slapd?



---

**Cursor**

Great question! In a BCM-managed environment with nslcd/slapd for user management, we should definitely **NOT** use `useradd` for creating system users for the exporters. Here's why and what we should do instead:

## Why not `useradd`:
- BCM manages user accounts centrally through LDAP/slapd
- Local `useradd` users would be inconsistent with the BCM management approach
- They might not persist across system updates or BCM reconfigurations
- Could conflict with BCM's user management policies

## Recommended approach:
We should create **system service accounts** that are local to this specific node for running the exporters. These should be:

1. **Local system accounts** (not LDAP-managed) since they're for system services
2. **Created with minimal privileges** - just enough to run the exporters
3. **Not interactive accounts** - no shell, no home directory
4. **Documented in BCM** as part of the jobstats deployment

Let me check what user management approach is currently in place and then create the appropriate system accounts:


I can see that nslcd is running but having connectivity issues with the LDAP server. For the jobstats exporters, I recommend creating **local system accounts** that are independent of the LDAP infrastructure. This is the standard approach for system services.

Let me create the necessary system accounts and proceed with the installation:


Great! The prometheus user already exists. Now let me install the exporters and create the systemd services:


It looks like the cgroup_exporter might already be running. Let me check and then copy the files individually:


I see that all three exporters are already running! It looks like they might have been installed previously. Let me check if they're running as systemd services and what their current configuration is:


Perfect! The exporters are running and listening on the correct ports, but they're not managed by systemd. Let me create proper systemd service files for them and then we can manage them properly. First, let me stop the current processes and create the service files:


```plaintext
[Unit]
Description=Cgroup Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/cgroup_exporter --config.paths /slurm --collect.fullslurm --web.listen-address=:9306
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```plaintext
[Unit]
Description=Node Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/node_exporter --web.listen-address=:9100
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```plaintext
[Unit]
Description=NVIDIA GPU Prometheus Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/nvidia_gpu_prometheus_exporter --web.listen-address=:9445
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

Now let me reload systemd and start the services:


I see the issue - the ports are still in use from the previous processes. Let me check what's using them and clean up:


The processes are still running. Let me kill them properly and then start the systemd services:


Excellent! All three exporters are now running as systemd services. Let me update the todo list and test the other exporters:


Perfect! All exporters are working. Now let me install Prometheus. First, let me check if it's already installed:


Prometheus is installed but not running as a service. Let me download and install the latest version, then create a systemd service for it:


Good, we have a recent version of Prometheus. Now let me create the Prometheus configuration file:


```yaml
global:
  scrape_interval: 30s
  evaluation_interval: 30s
  external_labels:
    monitor: 'jobstats-dgx'

rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'dgx-node'
    scrape_interval: 30s
    scrape_timeout: 30s
    static_configs:
      - targets: ['localhost:9100', 'localhost:9306', 'localhost:9445']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: '^go_.*'
        action: drop
```

Now let me create the systemd service file for Prometheus:


```plaintext
[Unit]
Description=Prometheus Server for Jobstats
Documentation=https://prometheus.io/docs/introduction/overview/
Wants=network-online.target
After=network-online.target

[Service]
Type=notify
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090 \
    --web.enable-lifecycle

ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

Great! Prometheus is starting up. Let me test if it's accessible and then proceed with Grafana installation:


Perfect! Prometheus is working and all targets are healthy. Let me install Grafana using the official repository:


Grafana is installed! Let me start it and configure it:


Let me check what Grafana service is available:


Let me check what Grafana files were installed and find the correct service name:


I see there's an init.d script. Let me check if there's a systemd service or create one:


Great! Let me enable and start the service:


Excellent! Grafana is running. Now let me update the todo list and install the jobstats command:


Now let me install the jobstats command:


Now let me configure the jobstats config.py file for this environment:


Now let me update the config.py file for this DGX environment:


```python
##########################
## JOBSTATS CONFIG FILE ##
##########################

# prometheus server address, port, and retention period
PROM_SERVER = "http://localhost:9090"
PROM_RETENTION_DAYS = 365

# external MariaDB config used to write instead of writing in Slurm DB (optional)
# if using Slurm DB then include the lines below with "enabled": False
EXTERNAL_DB_TABLE = "job_statistics"
EXTERNAL_DB_CONFIG = {
    "enabled": False,  # set to True to use the external db for storing stats
    "host": "127.0.0.1",
    "port": 3307,
    "database": "jobstats",
    "user": "jobstats",
    "password": "password",
#     "config_file": "/path/to/jobstats-db.cnf"
}

# number of seconds between measurements
SAMPLING_PERIOD = 30

# threshold values for red versus black text and notes
GPU_UTIL_RED   = 15  # percentage
GPU_UTIL_BLACK = 25  # percentage
CPU_UTIL_RED   = 65  # percentage
CPU_UTIL_BLACK = 80  # percentage
TIME_EFFICIENCY_RED   = 10  # percentage
TIME_EFFICIENCY_BLACK = 60  # percentage
MIN_MEMORY_USAGE      = 70  # percentage
MIN_RUNTIME_SECONDS   = 10 * SAMPLING_PERIOD  # seconds

# translate cluster names in Slurm DB to informal names
CLUSTER_TRANS = {}  # if no translations then use an empty dictionary
CLUSTER_TRANS_INV = dict(zip(CLUSTER_TRANS.values(), CLUSTER_TRANS.keys()))

# maximum number of characters to display in jobname
MAX_JOBNAME_LEN = 64


################################################################################
##                         C U S T O M    N O T E S                           ##
##                                                                            ##
##  Be sure to work from the examples. Pay attention to the different quote   ##
##  characters when f-strings are involved.                                   ##
################################################################################

NOTES = []

###############################
# B O L D   R E D   N O T E S #
###############################

# zero GPU utilization (single GPU jobs)
condition = 'self.js.gpus and (self.js.diff > c.MIN_RUNTIME_SECONDS) and num_unused_gpus > 0 ' \
            'and self.js.gpus == 1'
note = ("This job did not use the GPU. Please resolve this " \
        "before running additional jobs. Wasting resources " \
        "will cause your subsequent jobs to have a lower priority. " \
        "Is the code GPU-enabled? " \
        "Please consult the documentation for the software. For more info:",
        "https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing")
style = "bold-red"
NOTES.append((condition, note, style))

# zero GPU utilization (multi-GPU jobs)
condition = 'self.js.gpus and (self.js.diff > c.MIN_RUNTIME_SECONDS) and num_unused_gpus > 0 ' \
            'and self.js.gpus > 1'
note = ('f"This job did not use {num_unused_gpus} of the {self.js.gpus} allocated GPUs. "' \
        '"Please resolve this before running additional jobs. "' \
        'f"Wasting resources will cause your subsequent jobs to have a lower priority. {multi}"' \
        '"Please consult the documentation for the software. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing")
style = "bold-red"
NOTES.append((condition, note, style))

# zero CPU utilization (single node)
condition = '(self.js.diff > c.MIN_RUNTIME_SECONDS) and (num_unused_nodes > 0) ' \
            'and (self.js.nnodes == "1")'
note = ('"This job did not use the CPU. This suggests that something went wrong "' \
        '"at the very beginning of the job. For batch jobs, check your Slurm and "' \
        '"application scripts for errors and look for useful information in the "' \
        'f"file slurm-{self.js.jobid}.out if it exists."')
style = "bold-red"
NOTES.append((condition, note, style))

# zero CPU utilization (multiple nodes)
condition = '(self.js.diff > c.MIN_RUNTIME_SECONDS) and (num_unused_nodes > 0) ' \
            'and (int(self.js.nnodes) > 1)'
note = ('f"This job did not use {num_unused_nodes} of the {self.js.nnodes} allocated "' \
        'f"nodes. Please resolve this before running additional jobs. {multi_cpu}Please "' \
        '"consult the documentation for the software. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code")
style = "bold-red"
NOTES.append((condition, note, style))

# jobs that should have used 10 GB MIG (batch jobs)
condition = '(not zero_gpu) and (self.js.gpus == 1) and (self.js.cluster == "della") and ' \
            '(self.js.partition == "gpu") and (self.js.diff > c.MIN_RUNTIME_SECONDS) and ' \
            '(self.js.gpu_utilization < c.GPU_UTIL_RED) and (int(self.js.ncpus) == 1) and ' \
            '(gb_per_node_used < 32) and (gpu_mem_used < 10) and ' \
            '(self.js.jobname != "interactive") and ' \
            '(not self.js.jobname.startswith("sys/dashboard/sys/jupyter"))'
note = ("This job should probably use a 10 GB MIG GPU instead of a full A100 GPU. MIG is " \
        "ideal for jobs with a low GPU utilization that only require a single CPU-core, " \
        "less than 32 GB of CPU memory, and less than 10 GB of GPU memory. For " \
        "future jobs, please add the following line to your Slurm script:",
        "#SBATCH --partition=mig",
        "For more info:",
        "https://researchcomputing.princeton.edu/systems/della#gpus")
style = "bold-red"
NOTES.append((condition, note, style))

# jobs that should have used 10 GB MIG (salloc jobs)
condition = '(not zero_gpu) and (self.js.gpus == 1) and (self.js.cluster == "della") and ' \
            '(self.js.partition in ["gpu", "gputest"]) and (self.js.diff > c.MIN_RUNTIME_SECONDS) and ' \
            '(self.js.gpu_utilization < c.GPU_UTIL_RED) and (int(self.js.ncpus) == 1) and ' \
            '(gb_per_node_used < 32) and (gpu_mem_used < 10) and ' \
            '(self.js.jobname == "interactive")'
note = ("This job should probably use a 10 GB MIG GPU instead of a full A100 GPU. MIG is " \
        "ideal for jobs with a low GPU utilization that only require a single CPU-core, " \
        "less than 32 GB of CPU memory, and less than 10 GB of GPU memory. To use MIG " \
        "with \"salloc\":",
        "$ salloc --nodes=1 --ntasks=1 --time=60:00 --gres=gpu:1 --partition=mig",
        "For more info:",
        "https://researchcomputing.princeton.edu/systems/della#gpus")
style = "bold-red"
NOTES.append((condition, note, style))

# jobs that should have used 10 GB MIG (jupyter ondemand jobs)
condition = '(not zero_gpu) and (self.js.gpus == 1) and (self.js.cluster == "della") and ' \
            '(self.js.partition in ["gpu", "gputest"]) and (self.js.diff > c.MIN_RUNTIME_SECONDS) and ' \
            '(self.js.gpu_utilization < c.GPU_UTIL_RED) and (int(self.js.ncpus) == 1) and ' \
            '(gb_per_node_used < 32) and (gpu_mem_used < 10) and ' \
            '(self.js.jobname.startswith("sys/dashboard/sys/jupyter"))'
note = ("This job should probably use a 10 GB MIG GPU instead of a full A100 GPU. MIG is " \
        "ideal for jobs with a low GPU utilization that only require a single CPU-core, " \
        "less than 32 GB of CPU memory, and less than 10 GB of GPU memory. To use MIG " \
        "with OnDemand Jupyter, choose \"mig\" as the \"Custom partition\" when " \
        "creating the session. For more info:",
        "https://researchcomputing.princeton.edu/support/knowledge-base/jupyter#environments")
style = "bold-red"
NOTES.append((condition, note, style))

# low GPU utilization (ondemand and salloc)
condition = '(not zero_gpu) and self.js.gpus and (self.js.gpu_utilization <= c.GPU_UTIL_RED) ' \
            'and interactive_job and (self.js.diff / SECONDS_PER_HOUR > 8)'
note = ('f"The overall GPU utilization of this job is only {round(self.js.gpu_utilization)}%. "' \
        '"This value is low compared to the cluster mean value of 50%. Please "' \
        '"do not create \'salloc\' or OnDemand sessions for more than 8 hours unless you "' \
        '"plan to work intensively during the entire period."')
style = "bold-red"
NOTES.append((condition, note, style))

# low GPU utilization (batch jobs)
condition = '(not zero_gpu) and self.js.gpus and (self.js.gpu_utilization <= c.GPU_UTIL_RED) ' \
            'and (not interactive_job)'
note = ('f"The overall GPU utilization of this job is only {round(self.js.gpu_utilization)}%. "' \
        '"This value is low compared to the cluster mean value of 50%. Please "' \
        '"investigate the reason for the low utilization. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#util")
style = "bold-red"
NOTES.append((condition, note, style))

# low CPU utilization (red, more than one core)
condition = '(not zero_cpu) and (not self.js.gpus) and (self.js.cpu_efficiency < c.CPU_UTIL_RED) ' \
            'and (int(self.js.ncpus) > 1)'
note = ('f"The overall CPU utilization of this job is {ceff}%. This value "' \
        'f"is{somewhat}low compared to the target range of "' \
        'f"90% and above. Please investigate the reason for the low efficiency. "' \
        '"For instance, have you conducted a scaling analysis? For more info:"',
        "https://researchcomputing.princeton.edu/get-started/cpu-utilization")
style = "bold-red"
NOTES.append((condition, note, style))

# low CPU utilization (red, serial job)
condition = '(not zero_cpu) and (not self.js.gpus) and (self.js.cpu_efficiency < c.CPU_UTIL_RED) ' \
            'and (int(self.js.ncpus) == 1)'
note = ('f"The overall CPU utilization of this job is {ceff}%. This value "' \
        'f"is{somewhat}low compared to the target range of "' \
        'f"90% and above. Please investigate the reason for the low efficiency. "' \
        '"For more info:"',
        "https://researchcomputing.princeton.edu/get-started/cpu-utilization")
style = "bold-red"
NOTES.append((condition, note, style))

# out of memory
condition = 'self.js.state == "OUT_OF_MEMORY" and (not zero_cpu)'
note = ("This job failed because it needed more CPU memory than the amount that " \
        "was requested. If there are no other problems then the solution is to " \
        "resubmit the job while requesting more CPU memory by " \
        "modifying the --mem-per-cpu or --mem Slurm directive. For more info: ",
        "https://researchcomputing.princeton.edu/support/knowledge-base/memory")
style = "bold-red"
NOTES.append((condition, note, style))

# timeout
condition = '(self.js.state == "TIMEOUT") and (not zero_gpu) and (not zero_cpu)'
note = ("This job failed because it exceeded the time limit. If there are no " \
        "other problems then the solution is to increase the value of the " \
        "--time Slurm directive and resubmit the job. For more info:",
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "bold-red"
NOTES.append((condition, note, style))

# excessive run time limit (red)
condition = 'self.js.time_eff_violation and self.js.time_efficiency <= c.TIME_EFFICIENCY_RED ' \
            'and (not zero_gpu) and (not zero_cpu)'
note = ('f"This job only needed {self.js.time_efficiency}% of the requested time "' \
        'f"which was {self.human_seconds(SECONDS_PER_MINUTE * self.js.timelimitraw)}. "' \
        '"For future jobs, please request less time by modifying "' \
        '"the --time Slurm directive. This will "' \
        '"lower your queue times and allow the Slurm job scheduler to work more "' \
        '"effectively for all users. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "bold-red"
NOTES.append((condition, note, style))

#########################
# P L A I N   N O T E S #
#########################

# excessive run time limit (black)
condition = 'self.js.time_eff_violation and self.js.time_efficiency > c.TIME_EFFICIENCY_RED ' \
            'and (not zero_gpu) and (not zero_cpu)'
note = ('f"This job only needed {self.js.time_efficiency}% of the requested time "' \
        'f"which was {self.human_seconds(SECONDS_PER_MINUTE * self.js.timelimitraw)}. "' \
        '"For future jobs, please request less time by modifying "' \
        '"the --time Slurm directive. This will "' \
        '"lower your queue times and allow the Slurm job scheduler to work more "' \
        '"effectively for all users. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "normal"
NOTES.append((condition, note, style))

# somewhat low GPU utilization
condition = '(not zero_gpu) and self.js.gpus and (self.js.gpu_utilization < c.GPU_UTIL_BLACK) and ' \
            '(self.js.gpu_utilization > c.GPU_UTIL_RED) and (self.js.diff > c.MIN_RUNTIME_SECONDS)'
note = ('f"The overall GPU utilization of this job is {round(self.js.gpu_utilization)}%. "' \
        '"This value is somewhat low compared to the cluster mean value of 50%. For more info:"',
        'https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#util')
style = "normal"
NOTES.append((condition, note, style))

# low CPU utilization (black, more than one core)
condition = '(not zero_cpu) and (not self.js.gpus) and (self.js.cpu_efficiency <= c.CPU_UTIL_BLACK) ' \
            'and (self.js.cpu_efficiency > c.CPU_UTIL_RED) and int(self.js.ncpus) > 1'
note = ('f"The overall CPU utilization of this job is {ceff}%. This value "' \
        'f"is{somewhat}low compared to the target range of "' \
        'f"90% and above. Please investigate the reason for the low efficiency. "' \
        '"For instance, have you conducted a scaling analysis? For more info:"',
        "https://researchcomputing.princeton.edu/get-started/cpu-utilization")
style = "normal"
NOTES.append((condition, note, style))

# low CPU utilization (black, serial job)
condition = '(not zero_cpu) and (not self.js.gpus) and (self.js.cpu_efficiency <= c.CPU_UTIL_BLACK) ' \
            'and (self.js.cpu_efficiency > c.CPU_UTIL_RED) and int(self.js.ncpus) == 1'
note = ('f"The overall CPU utilization of this job is {ceff}%. This value "' \
        'f"is{somewhat}low compared to the target range of "' \
        'f"90% and above. Please investigate the reason for the low efficiency. "' \
        '"For more info:"',
        "https://researchcomputing.princeton.edu/get-started/cpu-utilization")
style = "normal"
NOTES.append((condition, note, style))

# CPU job that used too many nodes or too few cores per node (ignoring GPU jobs)
condition = '(int(self.js.nnodes) > 1) and (not self.js.gpus) and (self.js.cluster == "della") ' \
            'and (cores_per_node < 16) and (gb_per_node_used < 0.85 * 190 / int(self.js.nnodes))'
note = ('f"This job used {self.js.ncpus} CPU-cores from {self.js.nnodes} compute "' \
        '"nodes. Please try to use as few nodes as possible by decreasing "' \
        '"the value of the --nodes Slurm directive and increasing the value of --ntasks-per-node. "' \
        '"Run the \'shownodes\' command to see the number of "' \
        '"CPU-cores per node (see \'TOTAL CPUs\' column). Della has nodes with 192 "' \
        '"CPU-cores and 1 TB of memory. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "normal"
NOTES.append((condition, note, style))

condition = '(int(self.js.nnodes) > 1) and (not self.js.gpus) and (self.js.cluster == "stellar") ' \
            'and (cores_per_node < 48) and (gb_per_node_used < 0.85 * 512 / int(self.js.nnodes))'
note = ('f"This job used {self.js.ncpus} CPU-cores from {self.js.nnodes} compute "' \
        '"nodes. Please try to use as few nodes as possible by decreasing "' \
        '"the value of the --nodes Slurm directive and increasing the value of --ntasks-per-node. "' \
        '"Each Intel node (PU, PPPL) provides 96 CPU-cores and 760 GB of memory while "' \
        '"an AMD node (CIMES) offers 128 cores and 512 GB. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "normal"
NOTES.append((condition, note, style))

condition = '(int(self.js.nnodes) > 1) and (not self.js.gpus) and (self.js.cluster == "tiger3") ' \
            'and (cores_per_node < 56) and (gb_per_node_used < 0.85 * 1000 / int(self.js.nnodes))'
note = ('f"This job used {self.js.ncpus} CPU-cores from {self.js.nnodes} compute "' \
        '"nodes. Please try to use as few nodes as possible by decreasing "' \
        '"the value of the --nodes Slurm directive and increasing the value of --ntasks-per-node. "' \
        '"Each Tiger (CPU) node provides 112 CPU-cores and 1000 GB of memory. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "normal"
NOTES.append((condition, note, style))

condition = '(int(self.js.nnodes) > 1) and (not self.js.gpus) and (self.js.cluster == "adroit") ' \
            'and (cores_per_node < 16) and (gb_per_node_used < 0.85 * 384 / int(self.js.nnodes))'
note = ('f"This job used {self.js.ncpus} CPU-cores from {self.js.nnodes} compute "' \
        '"nodes. Please try to use as few nodes as possible by decreasing "' \
        '"the value of the --nodes Slurm directive and increasing the value of --ntasks-per-node. "' \
        '"Run the \'shownodes\' command to see the number of "' \
        '"CPU-cores per node (see \'TOTAL CPUs\' column). For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm")
style = "normal"
NOTES.append((condition, note, style))

# multinode GPU fragmentation
condition = '(int(self.js.nnodes) > 1) and self.js.gpus and (self.js.cluster == "della") and ' \
            '(int(self.js.nnodes) == self.js.gpus) and ("pli" not in self.js.partition)'
note = ('f"This job used {self.js.gpus} GPUs from {self.js.nnodes} compute nodes. "' \
        '"The nodes on the \'gpu\' partition of Della have either 2 or 4 GPUs per node. Please "' \
        '"try to use as few nodes as possible by lowering the value of the "' \
        '"--nodes Slurm directive and increasing the value of --gres=gpu:<N>. "' \
        '"For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm#gpus")
style = "normal"
NOTES.append((condition, note, style))

condition = '(self.js.cluster == "della") and ("pli" in self.js.partition) and ' \
            '(int(self.js.nnodes) > 1) and (self.js.gpus / int(self.js.nnodes) < 8) '
note = ('f"This job used {self.js.gpus} GPUs from {self.js.nnodes} compute nodes. "' \
        '"The PLI GPU nodes on Della have 8 GPUs per node. Please allocate "' \
        '"all of the GPUs within a node before splitting your job across "' \
        '"multiple nodes. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm#gpus")
style = "normal"
NOTES.append((condition, note, style))

condition = '(self.js.cluster == "tiger3") and (self.js.partition == "gpu") and ' \
            '(int(self.js.nnodes) > 1) and (self.js.gpus / int(self.js.nnodes) < 4) '
note = ('f"This job used {self.js.gpus} GPUs from {self.js.nnodes} compute nodes. "' \
        '"The H100 GPU nodes on Tiger have 4 GPUs per node. Please allocate "' \
        '"all of the GPUs within a node before splitting your job across "' \
        '"multiple nodes. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/slurm#gpus")
style = "normal"
NOTES.append((condition, note, style))

# too many CPU-cores per GPU
condition = '(self.js.cluster == "della") and ("pli" in self.js.partition) and ' \
            '(not zero_gpu) and (cores_per_gpu > 12)'
note = ("Each node on Della (PLI) has 96 CPU-cores and 8 GPUs. If possible " \
        "please try to allocate only up to 12 CPU-cores per GPU. This will " \
        "prevent the situation where there are free GPUs on a node but not " \
        "enough CPU-cores to accept new jobs. For more info:",
        "https://researchcomputing.princeton.edu/systems/della#pli")
style = "normal"
NOTES.append((condition, note, style))

condition = '(self.js.cluster == "tiger3") and (self.js.partition == "gpu") and ' \
            '(not zero_gpu) and (cores_per_gpu > 12)'
note = ("Each node on Tiger (gpu) has 112 CPU-cores and 4 GPUs. If possible " \
        "please try to allocate only up to 12 CPU-cores per GPU. This will " \
        "prevent the situation where there are free GPUs on a node but not " \
        "enough CPU-cores to accept new jobs. For more info:",
        "https://researchcomputing.princeton.edu/systems/tiger")
style = "normal"
NOTES.append((condition, note, style))

# too much CPU memory per GPU
condition = '(self.js.cluster == "della") and ("pli" in self.js.partition) and ' \
            '(not zero_gpu) and (cpu_mem_per_gpu > 115) and (total_used / total < 0.85)'
note = ("Each node on Della (PLI) has 1024 GB of CPU memory " \
        "and 8 GPUs. If possible please try to allocate only up to 115 GB of " \
        "CPU memory per GPU. This will prevent the situation where there " \
        "are free GPUs on a node but not enough CPU memory to accept new " \
        "jobs. For more info:",
        "https://researchcomputing.princeton.edu/systems/della#pli")
style = "normal"
NOTES.append((condition, note, style))

condition = '(self.js.cluster == "tiger3") and (self.js.partition == "gpu") and ' \
            '(not zero_gpu) and (cpu_mem_per_gpu > 240) and (total_used / total < 0.85)'
note = ("Each node on Tiger (gpu) has 1000 GB of CPU memory " \
        "and 4 GPUs. If possible please try to allocate only up to 240 GB of " \
        "CPU memory per GPU. This will prevent the situation where there " \
        "are free GPUs on a node but not enough CPU memory to accept new " \
        "jobs. For more info:",
        "https://researchcomputing.princeton.edu/systems/tiger")
style = "normal"
NOTES.append((condition, note, style))

# overallocating CPU memory (for CPU and GPU jobs)
"""Ignore jobs that use the default memory per core and jobs that almost allocate
   all of the CPU-cores. Also ignore GPU jobs that allocate less than 50 GB of
   CPU memory per GPU (would also be nice to ignore jobs that allocate all of the
   GPUs per node). DEFAULT_MEM_PER_CORE is the default CPU memory per core in
   bytes. If unsure what to use for this then use the memory per node divided by
   the number of cores per node."""
DEFAULT_MEM_PER_CORE = {"adroit":  3_300_000_000,
                        "della":   4_100_000_000,
                        "stellar": 7_600_000_000,
                        "tiger3":  8_100_000_000}
CORES_PER_NODE = {"adroit":64,
                  "della":192,
                  "stellar":96,
                  "tiger":112}
condition = '(not zero_gpu) and (not zero_cpu) and (self.js.cpu_memory_efficiency < c.MIN_MEMORY_USAGE) ' \
            'and (gb_per_core_total > mpc / 1024**3) and gpu_show and (not self.js.partition == "mig") and ' \
            '(self.js.state != "OUT_OF_MEMORY") and (cores_per_node < 0.85 * cpn) and ' \
            '(self.js.diff > c.MIN_RUNTIME_SECONDS)'
note = ('f"This job {opening} of the {self.cpu_memory_formatted(with_label=False)} "' \
        '"of total allocated CPU memory. "' \
        '"For future jobs, please allocate less memory by using a Slurm directive such "' \
        'f"as --mem-per-cpu={self.rounded_memory_with_safety(gb_per_core_used)}G or "' \
        'f"--mem={self.rounded_memory_with_safety(gb_per_node_used)}G. "' \
        '"This will reduce your queue times and make the resources available to "' \
        '"other jobs. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/memory")
style = "normal"
NOTES.append((condition, note, style))

# serial jobs wasting multiple CPU-cores
condition = '(self.js.nnodes == "1") and (int(self.js.ncpus) > 1) and (not self.js.gpus) and ' \
            '(serial_ratio > 0.85 and serial_ratio < 1.01)'
note = ('f"The CPU utilization of this job ({self.js.cpu_efficiency}%) is{approx}equal "' \
        '"to 1 divided by the number of allocated CPU-cores "' \
        'f"(1/{self.js.ncpus}={round(eff_if_serial, 1)}%). This suggests that you may be "' \
        '"running a code that can only use 1 CPU-core. If this is true then "' \
        '"allocating more than 1 CPU-core is wasteful. Please consult the "' \
        '"documentation for the software to see if it is parallelized. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code")
style = "normal"
NOTES.append((condition, note, style))

# jobs using less than half of a node on the Stellar cluster
condition = '(self.js.nnodes == "1") and (self.js.cluster == "stellar") and (not self.js.gpus) and ' \
            '(int(self.js.ncpus) < 48) and (not self.js.qos == "stellar-debug")'
note = ("The Stellar cluster is intended for jobs that require " \
        "multiple nodes. This job ran in the \"serial\" partition where jobs are " \
        "assigned the lowest priority. On Stellar, a job will run in the \"serial\" " \
        "partition if it requires 1 node and less than 48 CPU-cores. " \
        "Please consider carrying out this work elsewhere.")
style = "normal"
NOTES.append((condition, note, style))

# job ran using 40GB MIG GPU
MIG_NODES_1 = {"della-l01g3", "della-l01g4", "della-l01g5", "della-l01g6", "della-l01g7",
               "della-l01g8", "della-l01g9", "della-l01g10", "della-l01g11", "della-l01g12",
               "adroit-h11g2"}
condition = 'self.js.cluster == "della" and self.js.partition in ("gpu", "gputest") ' \
            'and self.js.is_mig_job()'
note = ("This job used a 40GB MIG GPU which work well for almost all codes. " \
        "A 40GB MIG GPU has half the compute power of a full A100 GPU. If " \
        "a full A100 is needed or if you suspect that MIG is causing problems " \
        "then see \"nomig\" on this webpage:",
        "https://researchcomputing.princeton.edu/systems/della#gpus")
style = "normal"
NOTES.append((condition, note, style))

# job ran using 10GB MIG GPU
MIG_NODES_2 = {"della-l01g1", "della-l01g2"}
condition = 'self.js.cluster == "della" and self.js.partition == "mig"'
note = ("This job ran on the \"mig\" partition where each job is limited to 1 MIG GPU, " \
        "1 CPU-core, 10 GB of GPU memory and 32 GB of CPU memory. A MIG GPU is " \
        "about 1/7th as powerful as an A100 GPU. Please continue to use the \"mig\" partition. " \
        "For more info:",
        'https://researchcomputing.princeton.edu/systems/della#gpus')
style = "normal"
NOTES.append((condition, note, style))

# job ran in the test queue
condition = '("test" in self.js.qos) or ("debug" in self.js.qos)'
note = ('f"This job ran in the {self.js.qos} QOS. Each user can only run a small number of "' \
        '"jobs simultaneously in this QOS. For more info:"',
        "https://researchcomputing.princeton.edu/support/knowledge-base/job-priority#test-queue")
style = "normal"
NOTES.append((condition, note, style))

# grafana URL via Open OnDemand helper app for Adroit cluster
condition = '(self.js.cluster == "adroit") and self.js.is_retained()'
note = ("See the URL below for various job metrics plotted as a function of time:",
        'f"https://myadroit.princeton.edu/pun/sys/jobstats/{self.js.jobid}  (VPN required off-campus)"')
style = "normal"
NOTES.append((condition, note, style))

# grafana URL via Open OnDemand helper app for Della cluster
condition = '(self.js.cluster == "della") and self.js.is_retained()'
note = ("See the URL below for various job metrics plotted as a function of time:",
        'f"https://mydella.princeton.edu/pun/sys/jobstats/{self.js.jobid}  (VPN required off-campus)"')
style = "normal"
NOTES.append((condition, note, style))

# grafana URL via Open OnDemand helper app for Stellar cluster
condition = '(self.js.cluster == "stellar") and self.js.is_retained()'
note = ("See the URL below for various job metrics plotted as a function of time:",
        'f"https://mystellar.princeton.edu/pun/sys/jobstats/{self.js.jobid}  (VPN required off-campus)"')
style = "normal"
NOTES.append((condition, note, style))

# grafana URL via Open OnDemand helper app for Tiger cluster
condition = '(self.js.cluster == "tiger") and self.js.is_retained()'
note = ("See the URL below for various job metrics plotted as a function of time:",
        'f"https://mytiger.princeton.edu/pun/sys/jobstats/{self.js.jobid}  (VPN required off-campus)"')
style = "normal"
NOTES.append((condition, note, style))

# example of a simple note that is always displayed
condition = 'True'
note = "Have a nice day!"
style = "normal"
NOTES.append((condition, note, style))

```

Now let me set up the Slurm prolog/epilog scripts for GPU tracking. First, let me check if we need to modify the BCM-managed configuration:


Now I need to check the current Slurm configuration to see if we need to add the prolog/epilog settings. Since this is BCM-managed, I need to ask you about this:

**Question for you:** I need to add the following lines to the Slurm configuration to enable the GPU tracking scripts:

```
Prolog=/etc/slurm/prolog.d/*.sh
Epilog=/etc/slurm/epilog.d/*.sh
```

Since these would go in the BCM-managed section of `slurm.conf`, I need you to add these through BCM. Can you add these lines to the Slurm configuration via BCM? The scripts are already installed in the correct locations.

Meanwhile, let me continue with the other setup tasks and create the documentation:


```plaintext
Prolog=/etc/slurm/prolog.d/*.sh
Epilog=/etc/slurm/epilog.d/*.sh
```

Now let me test the jobstats command and then create the comprehensive documentation:


Perfect! Now let me create the comprehensive documentation:


```markdown
# DGX Jobstats Deployment Guide

This document provides step-by-step instructions for deploying Princeton University's jobstats monitoring platform on a DGX system managed by BCM (Base Command Manager).

## Overview

Jobstats is a comprehensive job monitoring platform designed for CPU and GPU clusters using Slurm. It provides:
- Real-time job utilization metrics
- GPU and CPU performance monitoring
- Automated efficiency reports
- Web-based dashboards via Grafana
- Command-line tools for job analysis

## Architecture

The jobstats platform consists of several components:
- **Exporters**: Collect metrics from compute nodes
- **Prometheus**: Time-series database for metrics storage
- **Grafana**: Web interface for visualization
- **Jobstats Command**: CLI tool for job analysis
- **Slurm Integration**: Prolog/epilog scripts for job tracking

## Prerequisites

- DGX system with BCM management
- Slurm workload manager with cgroup support
- NVIDIA GPUs with nvidia-smi
- Go compiler (for building exporters)
- Python 3.6+ with requests and blessed libraries

## Step-by-Step Deployment

### 1. System Analysis and Preparation

The system was analyzed and found to have:
- ✅ Slurm with cgroup support already configured
- ✅ NVIDIA drivers and nvidia-smi available
- ✅ Go compiler installed
- ✅ BCM-managed configuration in `/cm/shared/apps/slurm/var/etc/slurm/`

**Key Configuration Files:**
- Slurm config: `/cm/shared/apps/slurm/var/etc/slurm/slurm.conf`
- Cgroup config: `/cm/shared/apps/slurm/var/etc/slurm/cgroup.conf`
- GRES config: `/cm/shared/apps/slurm/var/etc/slurm/gres.conf`

### 2. User Management

In BCM-managed environments, user management is handled through nslcd/slapd rather than local `/etc/passwd`. For system services, we created local system accounts:

```bash
# Prometheus user already existed
# Created directories for data storage
mkdir -p /var/lib/prometheus
chown prometheus:prometheus /var/lib/prometheus
```

### 3. Exporters Installation

#### 3.1 Cgroup Exporter
Built and installed the cgroup exporter for CPU job statistics:

```bash
cd /root/jobstats/cgroup_exporter
go build -o cgroup_exporter
cp cgroup_exporter /usr/local/bin/
```

**Systemd Service:** `/etc/systemd/system/cgroup_exporter.service`
```ini
[Unit]
Description=Cgroup Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/cgroup_exporter --config.paths /slurm --collect.fullslurm --web.listen-address=:9306
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

#### 3.2 Node Exporter
Built and installed the node exporter for system metrics:

```bash
cd /root/jobstats/node_exporter
make build
cp node_exporter /usr/local/bin/
```

**Systemd Service:** `/etc/systemd/system/node_exporter.service`
```ini
[Unit]
Description=Node Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/node_exporter --web.listen-address=:9100
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

#### 3.3 NVIDIA GPU Exporter
Built and installed the NVIDIA GPU exporter:

```bash
cd /root/jobstats/nvidia_gpu_prometheus_exporter
go build -o nvidia_gpu_prometheus_exporter
cp nvidia_gpu_prometheus_exporter /usr/local/bin/
```

**Systemd Service:** `/etc/systemd/system/nvidia_gpu_prometheus_exporter.service`
```ini
[Unit]
Description=NVIDIA GPU Prometheus Exporter for Jobstats
After=network.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/nvidia_gpu_prometheus_exporter --web.listen-address=:9445
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

#### 3.4 Enable and Start Exporters
```bash
systemctl daemon-reload
systemctl enable cgroup_exporter node_exporter nvidia_gpu_prometheus_exporter
systemctl start cgroup_exporter node_exporter nvidia_gpu_prometheus_exporter
```

### 4. Prometheus Installation and Configuration

#### 4.1 Prometheus Configuration
Created `/etc/prometheus/prometheus.yml`:
```yaml
global:
  scrape_interval: 30s
  evaluation_interval: 30s
  external_labels:
    monitor: 'jobstats-dgx'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'dgx-node'
    scrape_interval: 30s
    scrape_timeout: 30s
    static_configs:
      - targets: ['localhost:9100', 'localhost:9306', 'localhost:9445']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: '^go_.*'
        action: drop
```

#### 4.2 Prometheus Systemd Service
Created `/etc/systemd/system/prometheus.service`:
```ini
[Unit]
Description=Prometheus Server for Jobstats
Documentation=https://prometheus.io/docs/introduction/overview/
Wants=network-online.target
After=network-online.target

[Service]
Type=notify
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090 \
    --web.enable-lifecycle

ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

#### 4.3 Start Prometheus
```bash
chown -R prometheus:prometheus /etc/prometheus /var/lib/prometheus
systemctl daemon-reload
systemctl enable prometheus
systemctl start prometheus
```

### 5. Grafana Installation and Configuration

#### 5.1 Install Grafana
```bash
wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -
echo "deb https://packages.grafana.com/oss/deb stable main" > /etc/apt/sources.list.d/grafana.list
apt update
apt install -y grafana
```

#### 5.2 Start Grafana
```bash
systemctl enable grafana-server
systemctl start grafana-server
```

**Access Grafana:** http://localhost:3000
- Default username: `admin`
- Default password: `admin`

### 6. Jobstats Command Installation

#### 6.1 Install Dependencies
```bash
apt install -y python3-requests python3-blessed
```

#### 6.2 Install Jobstats Files
```bash
cp /root/jobstats/jobstats/jobstats /usr/local/bin/
cp /root/jobstats/jobstats/jobstats.py /usr/local/bin/
cp /root/jobstats/jobstats/output_formatters.py /usr/local/bin/
cp /root/jobstats/jobstats/config.py /usr/local/bin/
chmod +x /usr/local/bin/jobstats
```

#### 6.3 Configure Jobstats
Updated `/usr/local/bin/config.py`:
```python
# prometheus server address, port, and retention period
PROM_SERVER = "http://localhost:9090"
PROM_RETENTION_DAYS = 365
```

### 7. Slurm Integration

#### 7.1 GPU Tracking Scripts
Installed prolog/epilog scripts for GPU job tracking:

```bash
mkdir -p /etc/slurm/prolog.d /etc/slurm/epilog.d
cp /root/jobstats/jobstats/slurm/prolog.d/gpustats_helper.sh /etc/slurm/prolog.d/
cp /root/jobstats/jobstats/slurm/epilog.d/gpustats_helper.sh /etc/slurm/epilog.d/
chmod +x /etc/slurm/prolog.d/gpustats_helper.sh /etc/slurm/epilog.d/gpustats_helper.sh
```

#### 7.2 Slurm Configuration (BCM-Managed)
**REQUIRED BCM CONFIGURATION:** Add these lines to the BCM-managed section of `slurm.conf`:

```
Prolog=/etc/slurm/prolog.d/*.sh
Epilog=/etc/slurm/epilog.d/*.sh
EpilogSlurmctld=/usr/local/sbin/slurmctldepilog.sh
```

#### 7.3 Job Summary Script
```bash
cp /root/jobstats/jobstats/slurm/slurmctldepilog.sh /usr/local/sbin/
chmod +x /usr/local/sbin/slurmctldepilog.sh
```

### 8. Service Management

All services are managed via systemd:

```bash
# Check status of all services
systemctl status cgroup_exporter node_exporter nvidia_gpu_prometheus_exporter prometheus grafana-server

# Restart services if needed
systemctl restart cgroup_exporter node_exporter nvidia_gpu_prometheus_exporter prometheus grafana-server

# View logs
journalctl -u cgroup_exporter -f
journalctl -u node_exporter -f
journalctl -u nvidia_gpu_prometheus_exporter -f
journalctl -u prometheus -f
journalctl -u grafana-server -f
```

## Verification

### 1. Check Exporter Endpoints
```bash
# Node exporter
curl http://localhost:9100/metrics | head -5

# Cgroup exporter
curl http://localhost:9306/metrics | head -5

# NVIDIA GPU exporter
curl http://localhost:9445/metrics | head -5
```

### 2. Check Prometheus Targets
```bash
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[].health'
```

### 3. Test Jobstats Command
```bash
jobstats --help
```

### 4. Access Web Interfaces
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## Port Configuration

| Service | Port | Description |
|---------|------|-------------|
| node_exporter | 9100 | System metrics |
| cgroup_exporter | 9306 | CPU job metrics |
| nvidia_gpu_prometheus_exporter | 9445 | GPU metrics |
| prometheus | 9090 | Metrics database |
| grafana | 3000 | Web dashboard |

## BCM-Specific Considerations

### Configuration Management
- Slurm configuration is managed by BCM in `/cm/shared/apps/slurm/var/etc/slurm/`
- BCM autogenerated sections cannot be edited directly
- Prolog/epilog settings must be added through BCM interface

### User Management
- System uses nslcd/slapd for user authentication
- Local system accounts created for service users
- Prometheus user already existed in the system

### File Locations
- Slurm configs: `/cm/shared/apps/slurm/var/etc/slurm/`
- Service configs: `/etc/systemd/system/`
- Prometheus data: `/var/lib/prometheus/`
- Grafana data: `/var/lib/grafana/`

## Troubleshooting

### Common Issues

1. **Exporters not starting**
   - Check if ports are already in use: `netstat -tlnp | grep -E "(9100|9306|9445)"`
   - Verify user permissions: `ls -la /usr/local/bin/*exporter`

2. **Prometheus not scraping targets**
   - Check configuration: `promtool check config /etc/prometheus/prometheus.yml`
   - Verify targets are accessible: `curl http://localhost:9100/metrics`

3. **Grafana not accessible**
   - Check service status: `systemctl status grafana-server`
   - Verify port binding: `netstat -tlnp | grep 3000`

4. **Jobstats command errors**
   - Check Python dependencies: `python3 -c "import requests, blessed"`
   - Verify config file: `python3 -c "import config"`

### Log Locations
- Systemd logs: `journalctl -u <service-name>`
- Prometheus logs: `journalctl -u prometheus`
- Grafana logs: `/var/log/grafana/grafana.log`

## Next Steps

1. **Configure BCM**: Add the required Slurm prolog/epilog settings through BCM
2. **Import Grafana Dashboard**: Import the jobstats dashboard from the repository
3. **Test with Sample Jobs**: Run test jobs to verify metrics collection
4. **Configure Alerts**: Set up Grafana alerts for job efficiency issues
5. **User Training**: Provide documentation for users on using the jobstats command

## Additional Resources

- [Princeton Jobstats Repository](https://github.com/PrincetonUniversity/jobstats)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Slurm Cgroup Configuration](https://slurm.schedmd.com/cgroups.html)

## Support

For issues specific to this DGX deployment:
1. Check service logs using `journalctl`
2. Verify BCM configuration changes
3. Test individual components (exporters, Prometheus, Grafana)
4. Consult the original Princeton jobstats documentation

---

**Deployment completed on:** $(date)
**System:** DGX with BCM management
**Slurm Version:** $(sinfo --version)
**NVIDIA Driver:** $(nvidia-smi --query-driver-version --format=csv,noheader)
```

Let me also create a quick verification script and complete the final todo:


```shellscript
#!/bin/bash

echo "=== DGX Jobstats Verification Script ==="
echo "Date: $(date)"
echo

# Check systemd services
echo "1. Checking systemd services..."
services=("cgroup_exporter" "node_exporter" "nvidia_gpu_prometheus_exporter" "prometheus" "grafana-server")
for service in "${services[@]}"; do
    if systemctl is-active --quiet "$service"; then
        echo "✅ $service is running"
    else
        echo "❌ $service is not running"
    fi
done
echo

# Check ports
echo "2. Checking service ports..."
ports=("9100:node_exporter" "9306:cgroup_exporter" "9445:nvidia_gpu_exporter" "9090:prometheus" "3000:grafana")
for port_info in "${ports[@]}"; do
    port=$(echo $port_info | cut -d: -f1)
    service=$(echo $port_info | cut -d: -f2)
    if netstat -tlnp | grep -q ":$port "; then
        echo "✅ Port $port ($service) is listening"
    else
        echo "❌ Port $port ($service) is not listening"
    fi
done
echo

# Check metrics endpoints
echo "3. Checking metrics endpoints..."
endpoints=("http://localhost:9100/metrics:node_exporter" "http://localhost:9306/metrics:cgroup_exporter" "http://localhost:9445/metrics:nvidia_gpu_exporter")
for endpoint_info in "${endpoints[@]}"; do
    url=$(echo $endpoint_info | cut -d: -f1-3)
    service=$(echo $endpoint_info | cut -d: -f4)
    if curl -s "$url" | head -1 | grep -q "# HELP"; then
        echo "✅ $service metrics endpoint is responding"
    else
        echo "❌ $service metrics endpoint is not responding"
    fi
done
echo

# Check Prometheus targets
echo "4. Checking Prometheus targets..."
if curl -s http://localhost:9090/api/v1/targets | jq -r '.data.activeTargets[].health' | grep -q "up"; then
    echo "✅ Prometheus has healthy targets"
    echo "   Targets:"
    curl -s http://localhost:9090/api/v1/targets | jq -r '.data.activeTargets[] | "   - \(.labels.job): \(.health)"'
else
    echo "❌ Prometheus targets are not healthy"
fi
echo

# Check jobstats command
echo "5. Checking jobstats command..."
if command -v jobstats >/dev/null 2>&1; then
    echo "✅ jobstats command is available"
    echo "   Version: $(jobstats --help | head -1)"
else
    echo "❌ jobstats command is not available"
fi
echo

# Check Slurm integration
echo "6. Checking Slurm integration..."
if [ -f "/etc/slurm/prolog.d/gpustats_helper.sh" ] && [ -f "/etc/slurm/epilog.d/gpustats_helper.sh" ]; then
    echo "✅ GPU tracking scripts are installed"
else
    echo "❌ GPU tracking scripts are missing"
fi

if [ -f "/usr/local/sbin/slurmctldepilog.sh" ]; then
    echo "✅ Job summary script is installed"
else
    echo "❌ Job summary script is missing"
fi
echo

# Check BCM configuration requirement
echo "7. BCM Configuration Requirements:"
echo "   ⚠️  The following lines need to be added to slurm.conf via BCM:"
echo "      Prolog=/etc/slurm/prolog.d/*.sh"
echo "      Epilog=/etc/slurm/epilog.d/*.sh"
echo "      EpilogSlurmctld=/usr/local/sbin/slurmctldepilog.sh"
echo

echo "=== Verification Complete ==="
```

