#!/usr/bin/env python3
"""
PyTorch GPU Test Script for Jobstats Validation

This script creates a PyTorch workload that consumes GPU memory and compute resources
to generate meaningful metrics for jobstats validation. It's designed to use approximately
4GB of VRAM and run for a configurable duration.

Usage:
    python test_jobstats_pytorch.py [options]

Options:
    --duration DURATION     Duration in minutes (default: 5)
    --job-name NAME         Job name (default: pytorch-gpu-test)
    --partition PARTITION   Slurm partition (default: defq)
    --nodelist NODELIST     Specific node list (default: dgx-01)
    --username USERNAME     Username to run as (default: testuser)
    --interactive           Interactive mode for prompts
    --non-interactive       Non-interactive mode (use defaults)
    --dry-run              Show what would be done without executing
"""

import argparse
import subprocess
import sys
import os
import time
import tempfile
import json
from pathlib import Path

class PyTorchJobTest:
    def __init__(self, duration=5, job_name="pytorch-gpu-test", partition="defq", 
                 nodelist="dgx-01", username="testuser", interactive=False, 
                 non_interactive=False, dry_run=False):
        self.duration = duration
        self.job_name = job_name
        self.partition = partition
        self.nodelist = nodelist
        self.username = username
        self.interactive = interactive
        self.non_interactive = non_interactive
        self.dry_run = dry_run
        
        # Find Slurm commands
        self.sbatch_cmd = self._find_slurm_command('sbatch')
        self.squeue_cmd = self._find_slurm_command('squeue')
        self.sacct_cmd = self._find_slurm_command('sacct')
        
    def _find_slurm_command(self, cmd):
        """Find the full path to a Slurm command."""
        try:
            result = subprocess.run(['which', cmd], capture_output=True, text=True)
            if result.returncode == 0:
                return result.stdout.strip()
        except:
            pass
        
        # Fallback to common Slurm paths
        common_paths = [
            f'/cm/shared/apps/slurm/current/bin/{cmd}',
            f'/cm/shared/apps/slurm/23.11.10/bin/{cmd}',
            f'/usr/bin/{cmd}',
            f'/usr/local/bin/{cmd}'
        ]
        
        for path in common_paths:
            if os.path.exists(path):
                return path
                
        return cmd  # Return as-is if not found
    
    def get_user_input(self):
        """Get user input for job parameters."""
        if self.non_interactive:
            print(f"Non-interactive mode: Using defaults")
            print(f"  Duration: {self.duration} minutes")
            print(f"  Job Name: {self.job_name}")
            print(f"  Partition: {self.partition}")
            print(f"  Nodelist: {self.nodelist}")
            print(f"  Username: {self.username}")
            return
            
        if not self.interactive:
            return
            
        print("\n" + "="*60)
        print("PyTorch GPU Test Job Configuration")
        print("="*60)
        
        try:
            # Duration
            duration_input = input(f"Job duration in minutes [{self.duration}]: ").strip()
            if duration_input:
                self.duration = int(duration_input)
            
            # Job name
            job_name_input = input(f"Job name [{self.job_name}]: ").strip()
            if job_name_input:
                self.job_name = job_name_input
            
            # Partition
            partition_input = input(f"Slurm partition [{self.partition}]: ").strip()
            if partition_input:
                self.partition = partition_input
            
            # Nodelist
            nodelist_input = input(f"Node list [{self.nodelist}]: ").strip()
            if nodelist_input:
                self.nodelist = nodelist_input
            
            # Username
            username_input = input(f"Username [{self.username}]: ").strip()
            if username_input:
                self.username = username_input
                
        except KeyboardInterrupt:
            print("\nExiting...")
            sys.exit(1)
        except EOFError:
            print("\nNon-interactive mode detected, using defaults...")
    
    def create_pytorch_script(self):
        """Create the PyTorch test script."""
        pytorch_code = f'''#!/usr/bin/env python3
"""
PyTorch GPU Test Script
Generated by test_jobstats_pytorch.py
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import sys
import os

def test_gpu_memory_usage():
    """Test GPU memory usage with PyTorch operations."""
    print("PyTorch GPU Test Starting...")
    print(f"PyTorch version: {{torch.__version__}}")
    print(f"CUDA available: {{torch.cuda.is_available()}}")
    
    if not torch.cuda.is_available():
        print("ERROR: CUDA not available on this system!")
        return False
    
    device = torch.device('cuda:0')
    print(f"Using device: {{device}}")
    print(f"GPU name: {{torch.cuda.get_device_name(0)}}")
    print(f"GPU memory: {{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}} GB")
    
    # Create a model that will use approximately 4GB of VRAM
    print("\\nCreating neural network model...")
    
    # Model architecture designed to use ~4GB VRAM
    class GPUTestModel(nn.Module):
        def __init__(self):
            super(GPUTestModel, self).__init__()
            # Large layers to consume memory
            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
            self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
            self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
            self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)
            
            # Large fully connected layers
            self.fc1 = nn.Linear(512 * 7 * 7, 2048)
            self.fc2 = nn.Linear(2048, 1024)
            self.fc3 = nn.Linear(1024, 10)
            
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.5)
            
        def forward(self, x):
            x = self.relu(self.conv1(x))
            x = self.relu(self.conv2(x))
            x = self.relu(self.conv3(x))
            x = self.relu(self.conv4(x))
            x = x.view(x.size(0), -1)
            x = self.relu(self.fc1(x))
            x = self.dropout(x)
            x = self.relu(self.fc2(x))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    model = GPUTestModel().to(device)
    print(f"Model created and moved to {{device}}")
    
    # Create optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # Create large input tensors to consume memory
    batch_size = 32
    input_tensor = torch.randn(batch_size, 3, 224, 224, device=device)
    target_tensor = torch.randint(0, 10, (batch_size,), device=device)
    
    print(f"Input tensor shape: {{input_tensor.shape}}")
    print(f"Input tensor memory: {{input_tensor.element_size() * input_tensor.nelement() / 1024**2:.1f}} MB")
    
    # Monitor GPU memory usage
    def print_gpu_memory():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        cached = torch.cuda.memory_reserved(0) / 1024**3
        print(f"GPU Memory - Allocated: {{allocated:.2f}} GB, Cached: {{cached:.2f}} GB")
    
    print("\\nInitial GPU memory usage:")
    print_gpu_memory()
    
    # Training loop
    print(f"\\nStarting training loop for {{self.duration}} minutes...")
    start_time = time.time()
    iteration = 0
    
    try:
        while time.time() - start_time < {self.duration * 60}:
            iteration += 1
            
            # Forward pass
            optimizer.zero_grad()
            outputs = model(input_tensor)
            loss = criterion(outputs, target_tensor)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Print progress every 30 seconds
            if iteration % 100 == 0:
                elapsed = time.time() - start_time
                print(f"Iteration {{iteration}}, Loss: {{loss.item():.4f}}, "
                      f"Elapsed: {{elapsed/60:.1f}} min", end="")
                print_gpu_memory()
            
            # Update target tensor occasionally to vary workload
            if iteration % 50 == 0:
                target_tensor = torch.randint(0, 10, (batch_size,), device=device)
    
    except KeyboardInterrupt:
        print("\\nTraining interrupted by user")
    except Exception as e:
        print(f"\\nError during training: {{e}}")
        return False
    
    print(f"\\nTraining completed after {{iteration}} iterations")
    print(f"Total time: {{(time.time() - start_time)/60:.1f}} minutes")
    
    # Final memory usage
    print("\\nFinal GPU memory usage:")
    print_gpu_memory()
    
    # Cleanup
    del model, input_tensor, target_tensor, optimizer, criterion
    torch.cuda.empty_cache()
    
    print("\\nPyTorch GPU test completed successfully!")
    return True

if __name__ == "__main__":
    success = test_gpu_memory_usage()
    sys.exit(0 if success else 1)
'''
        
        return pytorch_code
    
    def create_job_script(self):
        """Create the Slurm job script."""
        pytorch_script = self.create_pytorch_script()
        
        job_script = f'''#!/bin/bash
#SBATCH --job-name={self.job_name}
#SBATCH --partition={self.partition}
#SBATCH --nodelist={self.nodelist}
#SBATCH --time={self.duration + 5}:00
#SBATCH --output=/tmp/{self.job_name}_%j.out
#SBATCH --error=/tmp/{self.job_name}_%j.err
#SBATCH --gres=gpu:1
#SBATCH --mem=8G
#SBATCH --cpus-per-task=4

echo "=========================================="
echo "PyTorch GPU Test Job Starting"
echo "=========================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_JOB_NODELIST"
echo "Partition: $SLURM_JOB_PARTITION"
echo "Duration: {self.duration} minutes"
echo "Username: {self.username}"
echo "=========================================="

# Set environment variables
export SLURM_CONF=/cm/shared/apps/slurm/var/etc/slurm/slurm.conf
export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID

# Load required modules
echo "Loading modules..."
module load slurm 2>/dev/null || true
module load cuda 2>/dev/null || true
module load python 2>/dev/null || true

# Check if Python is available
if ! command -v python3 &> /dev/null; then
    echo "ERROR: python3 not found"
    exit 1
fi

# Check if CUDA is available
if ! command -v nvidia-smi &> /dev/null; then
    echo "ERROR: nvidia-smi not found - CUDA not available"
    exit 1
fi

echo "\\nSystem Information:"
echo "Python version: $(python3 --version)"
echo "CUDA version: $(nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | head -1)"
echo "GPU information:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader

# Install PyTorch if not available
echo "\\nChecking PyTorch installation..."
if ! python3 -c "import torch" 2>/dev/null; then
    echo "Installing PyTorch..."
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 || {{
        echo "Failed to install PyTorch with pip3, trying pip..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 || {{
            echo "ERROR: Failed to install PyTorch"
            exit 1
        }}
    }}
else
    echo "PyTorch already installed"
fi

# Create and run the PyTorch test script
echo "\\nCreating PyTorch test script..."
cat > /tmp/pytorch_gpu_test.py << 'EOF'
{pytorch_script}
EOF

echo "Running PyTorch GPU test..."
python3 /tmp/pytorch_gpu_test.py

# Cleanup
rm -f /tmp/pytorch_gpu_test.py

echo "\\n=========================================="
echo "PyTorch GPU Test Job Completed"
echo "=========================================="
'''
        
        return job_script
    
    def submit_job(self):
        """Submit the job to Slurm."""
        job_script = self.create_job_script()
        
        if self.dry_run:
            print("\\n" + "="*60)
            print("DRY RUN - Job Script that would be submitted:")
            print("="*60)
            print(job_script)
            print("\\n" + "="*60)
            print("DRY RUN - Slurm command that would be executed:")
            print("="*60)
            print(f"{self.sbatch_cmd} --uid={self.username} /tmp/pytorch_job_script.sh")
            return None
        
        # Write job script to temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
            f.write(job_script)
            temp_script = f.name
        
        try:
            # Make script executable
            os.chmod(temp_script, 0o755)
            
            # Submit job
            print(f"\\nSubmitting PyTorch GPU test job...")
            print(f"Job Name: {self.job_name}")
            print(f"Partition: {self.partition}")
            print(f"Node: {self.nodelist}")
            print(f"Duration: {self.duration} minutes")
            print(f"Username: {self.username}")
            
            cmd = [self.sbatch_cmd, f'--uid={self.username}', temp_script]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                job_id = result.stdout.strip().split()[-1]
                print(f"\\n✓ Job submitted successfully!")
                print(f"Job ID: {job_id}")
                print(f"Job script: {temp_script}")
                return job_id
            else:
                print(f"\\n✗ Failed to submit job:")
                print(f"Error: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"\\n✗ Error submitting job: {e}")
            return None
        finally:
            # Clean up temporary file
            try:
                os.unlink(temp_script)
            except:
                pass
    
    def monitor_job(self, job_id):
        """Monitor the submitted job."""
        if not job_id:
            return
            
        print(f"\\nMonitoring job {job_id}...")
        print("\\nJob Status (refresh every 10 seconds):")
        print("-" * 50)
        
        try:
            while True:
                # Check job status
                cmd = [self.squeue_cmd, '-j', job_id, '--noheader']
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0 and result.stdout.strip():
                    status_line = result.stdout.strip()
                    print(f"\\r{status_line}", end='', flush=True)
                    
                    # Check if job is completed
                    if any(state in status_line for state in ['CD', 'CA', 'F', 'TO']):
                        print(f"\\n\\nJob {job_id} completed")
                        break
                else:
                    print(f"\\nJob {job_id} not found in queue")
                    break
                
                time.sleep(10)
                
        except KeyboardInterrupt:
            print(f"\\n\\nMonitoring interrupted. Job {job_id} may still be running.")
        except Exception as e:
            print(f"\\nError monitoring job: {e}")
    
    def show_results(self, job_id):
        """Show job results and instructions."""
        if not job_id:
            return
            
        print("\\n" + "="*60)
        print("Job Results and Next Steps")
        print("="*60)
        
        print(f"\\n1. Check job output:")
        print(f"   tail -f /tmp/{self.job_name}_*.out")
        print(f"   tail -f /tmp/{self.job_name}_*.err")
        
        print(f"\\n2. Check job details:")
        print(f"   {self.sacct_cmd} -j {job_id} --format=JobID,JobName,Partition,Account,AllocCPUS,State,ExitCode,Start,End,Elapsed,ReqMem,MaxRSS,MaxVMSize")
        
        print(f"\\n3. Test jobstats command:")
        print(f"   su - {self.username} -c 'module load slurm && SLURM_CONF=/cm/shared/apps/slurm/var/etc/slurm/slurm.conf jobstats {job_id} -c slurm'")
        
        print(f"\\n4. Check GPU metrics in Prometheus:")
        print(f"   curl -s 'http://statsrv:9090/api/v1/query?query=nvidia_gpu_memory_used_bytes{{jobid=\"{job_id}\"}}' | jq")
        
        print(f"\\n5. Check cgroup metrics:")
        print(f"   curl -s 'http://statsrv:9090/api/v1/query?query=cgroup_memory_rss_bytes{{jobid=\"{job_id}\"}}' | jq")
    
    def run(self):
        """Run the complete test."""
        print("PyTorch GPU Test for Jobstats Validation")
        print("=" * 50)
        
        # Get user input
        self.get_user_input()
        
        # Submit job
        job_id = self.submit_job()
        
        if job_id and not self.dry_run:
            # Monitor job
            self.monitor_job(job_id)
            
            # Show results
            self.show_results(job_id)

def main():
    parser = argparse.ArgumentParser(description='PyTorch GPU Test for Jobstats Validation')
    parser.add_argument('--duration', type=int, default=5, 
                       help='Duration in minutes (default: 5)')
    parser.add_argument('--job-name', default='pytorch-gpu-test',
                       help='Job name (default: pytorch-gpu-test)')
    parser.add_argument('--partition', default='defq',
                       help='Slurm partition (default: defq)')
    parser.add_argument('--nodelist', default='dgx-01',
                       help='Node list (default: dgx-01)')
    parser.add_argument('--username', default='testuser',
                       help='Username to run as (default: testuser)')
    parser.add_argument('--interactive', action='store_true',
                       help='Interactive mode for prompts')
    parser.add_argument('--non-interactive', action='store_true',
                       help='Non-interactive mode (use defaults)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show what would be done without executing')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.interactive and args.non_interactive:
        print("Error: Cannot specify both --interactive and --non-interactive")
        sys.exit(1)
    
    if not args.interactive and not args.non_interactive:
        args.interactive = True
    
    # Create and run test
    test = PyTorchJobTest(
        duration=args.duration,
        job_name=args.job_name,
        partition=args.partition,
        nodelist=args.nodelist,
        username=args.username,
        interactive=args.interactive,
        non_interactive=args.non_interactive,
        dry_run=args.dry_run
    )
    
    test.run()

if __name__ == '__main__':
    main()
