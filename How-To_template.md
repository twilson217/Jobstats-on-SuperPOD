# BCM Jobstats Guided Setup Document

**Generated on:** Tue Sep 30 04:45:11 PM PDT 2025
**Mode:** Dry Run
**Configuration:** automation/configs/config.json

This document provides a complete record of the jobstats deployment process.
It can be used as a reference for what was done or as a manual implementation guide.

## Table of Contents

1. [Setup Overview](#overview)
2. [CPU Job Statistics](#cpu-job-stats)
3. [GPU Job Statistics](#gpu-job-stats)
4. [Node Statistics](#node-stats)
5. [Job Summaries](#job-summaries)
6. [Prometheus](#prometheus)
7. [Grafana](#grafana)
8. [Open OnDemand Jobstats Helper](#ood)
9. [The jobstats Command](#jobstats-command)
10. [Additional BCM Configurations](#bcm-configurations)

---

## 1. Setup Overview

### Description

The jobstats platform provides comprehensive job monitoring for Slurm clusters.
This guided setup will walk you through each step of the deployment process.

### Setup Process Overview

1. Switch to cgroup-based job accounting
2. Setup exporters (cgroup, node, GPU) on compute nodes
3. Setup prolog/epilog scripts on GPU nodes
4. Setup Prometheus server and configuration
5. Setup slurmctld epilog for job summaries
6. Configure Grafana interface
7. Install jobstats command-line tool

### Reference Documentation

- Princeton University: https://princetonuniversity.github.io/jobstats/setup/overview/

## 2. CPU Job Statistics

### Description

Configure Slurm for cgroup-based job accounting and install cgroup exporter

This section configures Slurm settings for cgroup accounting and installs
the cgroup_exporter for collecting cgroup metrics.

### What we'll do

- 2a. Configure slurm.conf settings (outside autogenerated section)
- 2b. Configure cgroup constraints via BCM cmsh
- 2c. Deploy cgroup_exporter

### 2a. Configure slurm.conf settings (outside autogenerated section)

#### Slurm Configuration for Cgroups

We need to configure Slurm to use cgroup-based job accounting.
These settings must be placed outside the AUTOGENERATED SECTION.

### Commands for Slurm Configuration

#### Host: slogin

```bash
# Step 1 - Backup slurm.conf before modification

cp /cm/shared/apps/slurm/var/etc/slurm/slurm.conf /cm/shared/apps/slurm/var/etc/slurm/slurm.conf.backup

# Step 2 - Update slurm.conf with cgroup settings

cat > /tmp/update_slurm_conf.py << 'EOF'
import re
import sys

# Read the file
with open('/cm/shared/apps/slurm/var/etc/slurm/slurm.conf', 'r') as f:
    content = f.read()

# Settings to update
settings = {
    'JobAcctGatherType': 'jobacct_gather/cgroup',
    'ProctrackType': 'proctrack/cgroup', 
    'TaskPlugin': 'affinity,cgroup'
}

# Find the AUTOGENERATED SECTION
autogen_match = re.search(r'^# BEGIN AUTOGENERATED SECTION.*$', content, re.MULTILINE)
if not autogen_match:
    print('ERROR: AUTOGENERATED SECTION not found')
    sys.exit(1)

autogen_pos = autogen_match.start()

# Process each setting
for key, value in settings.items():
    # Look for existing setting
    pattern = rf'^{re.escape(key)}\s*=.*$'
    match = re.search(pattern, content, re.MULTILINE)
    
    if match:
        # Replace existing setting
        old_line = match.group(0)
        new_line = f'{key}={value}'
        content = content.replace(old_line, new_line)
        print(f'Updated {key}={value}')
    else:
        # Insert new setting before AUTOGENERATED SECTION
        new_line = f'{key}={value}'
        content = content[:autogen_pos] + new_line + '\n' + content[autogen_pos:]
        print(f'Added {key}={value}')

# Write the updated file
with open('/cm/shared/apps/slurm/var/etc/slurm/slurm.conf', 'w') as f:
    f.write(content)

print('slurm.conf updated successfully')
EOF
python3 /tmp/update_slurm_conf.py

```

---

#### BCM Configuration Commands

The following BCM commands configure the slurmctld epilog:

```bash
cmsh -c "wlm;use slurm;set epilogslurmctld /usr/local/sbin/slurmctldepilog.sh;commit"
```

**Note:** BCM automatically manages prolog/epilog settings in slurm.conf.
The scripts we installed will be automatically discovered and executed.

### 2b. Configure cgroup constraints via BCM

#### BCM Cgroup Configuration

We need to configure three BCM settings to enable cgroup support:

1. **ConstrainRAMSpace**: Enable memory constraints for jobs
2. **ConstrainCores**: Enable CPU core constraints for jobs
3. **SelectTypeParameters**: Set to `CR_CPU_Memory` for cgroup resource tracking

### Commands for BCM Cgroup Configuration

#### Host: slogin

```bash
# Step 1 - Enable RAM space constraints for slurm cluster via BCM

cmsh -c "wlm;use slurm;cgroups;set constrainramspace yes;commit"

# Step 2 - Enable CPU core constraints for slurm cluster via BCM

cmsh -c "wlm;use slurm;cgroups;set constraincores yes;commit"

# Step 3 - Configure select type parameters for cgroup support on slurm cluster via BCM

cmsh -c "wlm;use slurm;set selecttypeparameters CR_CPU_Memory;commit"

```

---

### 2c. Deploy cgroup_exporter

### Commands for Cgroup Exporter Installation

#### Host: dgx-01

```bash
# Step 1 - Create working directory

mkdir -p /opt/jobstats-deployment

# Step 2 - Install Go compiler

apt update && apt install -y golang-go

# Step 3 - Clone or update cgroup_exporter

cd /opt/jobstats-deployment && if [ -d cgroup_exporter ]; then cd cgroup_exporter && git pull; else git clone https://github.com/plazonic/cgroup_exporter.git; fi

# Step 4 - Build cgroup_exporter

cd /opt/jobstats-deployment/cgroup_exporter && go build

# Step 5 - Stop cgroup_exporter service (if running)

systemctl stop cgroup_exporter || true && sleep 2

# Step 6 - Install cgroup_exporter binary

cp /opt/jobstats-deployment/cgroup_exporter/cgroup_exporter /usr/local/bin/

# Step 7 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 8 - Create cgroup_exporter systemd service

# Create systemd service for cgroup_exporter
cat > /etc/systemd/system/cgroup_exporter.service << 'EOF'
[Unit]
Description=Cgroup Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/cgroup_exporter --web.listen-address=:9306 --config.paths /slurm --collect.fullslurm
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Step 9 - Start cgroup_exporter service

systemctl daemon-reload && systemctl enable cgroup_exporter && systemctl start cgroup_exporter

# Step 10 - Set capabilities for cgroup_exporter to read procfs

setcap cap_sys_ptrace=eip /usr/local/bin/cgroup_exporter

```

---

### Commands for Cgroup Exporter Verification

#### Host: dgx-01

```bash
# Step 1 - Check cgroup_exporter service status

systemctl is-active cgroup_exporter

# Step 2 - Test cgroup_exporter metrics endpoint

curl -s http://localhost:9306/metrics | head -5

# Step 3 - Verify cgroup_exporter is listening on port 9306

netstat -tlnp | grep :9306 || ss -tlnp | grep :9306

```

---

## 3. GPU Job Statistics

### Description

This section sets up GPU monitoring and the necessary scripts
to track which GPUs are assigned to which jobs.

### What we'll do

- 3a. Deploy nvidia_gpu_prometheus_exporter
- 3b. GPU Job Ownership Helper (prolog/epilog scripts)

### 3a. Deploy nvidia_gpu_prometheus_exporter

### Commands for NVIDIA GPU Exporter Installation

#### Host: dgx-01

```bash
# Step 1 - Create working directory

mkdir -p /opt/jobstats-deployment

# Step 2 - Install Go compiler

apt update && apt install -y golang-go

# Step 3 - Clone or update NVIDIA GPU exporter

cd /opt/jobstats-deployment && if [ -d nvidia_gpu_prometheus_exporter ]; then cd nvidia_gpu_prometheus_exporter && git pull; else git clone https://github.com/plazonic/nvidia_gpu_prometheus_exporter.git; fi

# Step 4 - Build NVIDIA GPU exporter

cd /opt/jobstats-deployment/nvidia_gpu_prometheus_exporter && go build

# Step 5 - Stop nvidia_gpu_exporter service (if running)

systemctl stop nvidia_gpu_exporter || true && sleep 2

# Step 6 - Install NVIDIA GPU exporter binary

cp /opt/jobstats-deployment/nvidia_gpu_prometheus_exporter/nvidia_gpu_prometheus_exporter /usr/local/bin/

# Step 7 - Create NVIDIA GPU exporter systemd service

# Create systemd service for NVIDIA GPU exporter
cat > /tmp/nvidia_gpu_exporter.service << 'EOF'
[Unit]
Description=NVIDIA GPU Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/nvidia_gpu_prometheus_exporter --web.listen-address=:9445
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
cp /tmp/nvidia_gpu_exporter.service /etc/systemd/system/
rm /tmp/nvidia_gpu_exporter.service

# Step 8 - Start NVIDIA GPU exporter service

systemctl daemon-reload && systemctl enable nvidia_gpu_exporter && systemctl start nvidia_gpu_exporter

# Step 9 - Set capabilities for nvidia_gpu_prometheus_exporter to read procfs

setcap cap_sys_ptrace=eip /usr/local/bin/nvidia_gpu_prometheus_exporter

```

---

### Commands for GPU Exporter Verification

#### Host: dgx-01

```bash
# Step 1 - Check NVIDIA GPU exporter service status

systemctl is-active nvidia_gpu_exporter

# Step 2 - Test NVIDIA GPU exporter metrics endpoint

curl -s http://localhost:9445/metrics | head -5

# Step 3 - Verify NVIDIA GPU exporter is listening on port 9445

netstat -tlnp | grep :9445 || ss -tlnp | grep :9445

```

---

### 3b. GPU Job Ownership Helper (prolog/epilog scripts)

#### BCM Script Discovery System

BCM uses a generic prolog/epilog system that automatically calls all scripts
in specific directories. We'll install jobstats scripts using BCM's pattern:

- **Script locations:** `/cm/local/apps/slurm/var/prologs/` and `/cm/local/apps/slurm/var/epilogs/`
- **Shared storage:** `/cm/shared/apps/slurm/var/cm/` (accessible to all nodes)
- **Symlink pattern:** Local symlinks point to shared storage scripts
- **Naming convention:** Use `60-` prefix to run after existing BCM scripts

### Commands for Repository Cloning

#### Host: slogin

```bash
# Step 1 - Create working directory for jobstats deployment

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone or update jobstats repository

cd /opt/jobstats-deployment && if [ -d jobstats ]; then cd jobstats && git pull; else git clone https://github.com/PrincetonUniversity/jobstats.git; fi

```

---

### Commands for BCM Script Installation

#### Host: slogin

```bash
# Step 1 - Create shared storage directory for jobstats scripts

mkdir -p /cm/shared/apps/slurm/var/cm

# Step 2 - Copy prolog script to shared storage

cp /opt/jobstats-deployment/jobstats/slurm/prolog.d/gpustats_helper.sh /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh

# Step 3 - Make prolog script executable

chmod +x /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh

# Step 4 - Copy epilog script to shared storage

cp /opt/jobstats-deployment/jobstats/slurm/epilog.d/gpustats_helper.sh /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh

# Step 5 - Make epilog script executable

chmod +x /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh

# Step 6 - Create local prolog directory

mkdir -p /cm/local/apps/slurm/var/prologs

# Step 7 - Create local epilog directory

mkdir -p /cm/local/apps/slurm/var/epilogs

# Step 8 - Create prolog symlink (60- prefix for execution order)

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 9 - Create epilog symlink (60- prefix for execution order)

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

# Step 10 - Create prolog/epilog directories

mkdir -p /cm/local/apps/slurm/var/prologs /cm/local/apps/slurm/var/epilogs

# Step 11 - Create prolog symlink

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 12 - Create epilog symlink

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

```

---

#### Host: BCM Headnode

```bash
# Step 1 - Create prolog/epilog directories on BCM headnode

mkdir -p /cm/local/apps/slurm/var/prologs /cm/local/apps/slurm/var/epilogs

# Step 2 - Create prolog symlink on BCM headnode

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 3 - Create epilog symlink on BCM headnode

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

```

---

## 4. Node Statistics

### Description

This section installs the Prometheus node_exporter
on all compute nodes to collect system metrics.

### What we'll do

- Clone node_exporter repository
- Build and install node_exporter
- Create systemd service for node_exporter
- Start and enable service
- Verify node_exporter is running and collecting metrics

### Commands for Node Exporter Installation

#### Host: dgx-01

```bash
# Step 1 - Clone or update node_exporter

cd /opt/jobstats-deployment && if [ -d node_exporter ]; then cd node_exporter && git pull; else git clone https://github.com/prometheus/node_exporter.git; fi

# Step 2 - Build node_exporter

cd /opt/jobstats-deployment/node_exporter && make build

# Step 3 - Stop node_exporter service (if running)

systemctl stop node_exporter || true && sleep 2

# Step 4 - Install node_exporter binary

cp /opt/jobstats-deployment/node_exporter/node_exporter /usr/local/bin/

# Step 5 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 6 - Create node_exporter systemd service

# Create systemd service for node_exporter
cat > /etc/systemd/system/node_exporter.service << 'EOF'
[Unit]
Description=Node Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/node_exporter --web.listen-address=:9100
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Step 7 - Start node_exporter service

systemctl daemon-reload && systemctl enable node_exporter && systemctl start node_exporter

```

---

### Commands for Node Exporter Verification

#### Host: dgx-01

```bash
# Step 1 - Check node_exporter service status

systemctl is-active node_exporter

# Step 2 - Test node_exporter metrics endpoint

curl -s http://localhost:9100/metrics | head -5

# Step 3 - Verify node_exporter is listening on port 9100

netstat -tlnp | grep :9100 || ss -tlnp | grep :9100

```

---

## 5. Job Summaries

### Description

This section sets up the slurmctld epilog script
to generate and store job summaries in the Slurm database.

### What we'll do

- Install slurmctld epilog script
- Configure BCM epilogslurmctld setting

### Commands for Repository Cloning

#### Host: slogin

```bash
# Step 1 - Create working directory for jobstats deployment

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone or update jobstats repository

cd /opt/jobstats-deployment && if [ -d jobstats ]; then cd jobstats && git pull; else git clone https://github.com/PrincetonUniversity/jobstats.git; fi

```

---

### Commands for Slurmctld Epilog Installation

#### Host: slogin

```bash
# Step 1 - Copy slurmctld epilog script for job summaries

cp /opt/jobstats-deployment/jobstats/slurm/slurmctldepilog.sh /usr/local/sbin/

# Step 2 - Make slurmctld epilog script executable

chmod +x /usr/local/sbin/slurmctldepilog.sh

```

---

### Commands for BCM Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Configure BCM epilogslurmctld setting

cmsh -c "wlm;use slurm;set epilogslurmctld /usr/local/sbin/slurmctldepilog.sh;commit"

```

---

## 6. Prometheus

### Description

This section sets up the Prometheus server
to collect and store metrics from all exporters.

### What we'll do

- Download and install Prometheus
- Create Prometheus configuration
- Create systemd service for Prometheus
- Start and enable Prometheus service

#### Host: statsrv

```bash
# Step 1 - Download Prometheus

wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz

# Step 2 - Extract Prometheus

tar xzf prometheus-2.45.0.linux-amd64.tar.gz

# Step 3 - Stop Prometheus service (if running)

systemctl stop prometheus || true && sleep 2

# Step 4 - Install Prometheus binary

cp prometheus-2.45.0.linux-amd64/prometheus /usr/local/bin/

# Step 5 - Create Prometheus directories

mkdir -p /etc/prometheus /var/lib/prometheus

# Step 6 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 7 - Set ownership of data directory

chown prometheus:prometheus /var/lib/prometheus

```

---


### Prometheus Configuration

```yaml
global:
  scrape_interval: 30s
  evaluation_interval: 30s
  external_labels:
    monitor: 'jobstats-slurm'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node_exporter'
    static_configs:
      - targets: 
        - 'dgx-01:9100'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm
      - source_labels: [__name__]
        regex: '^go_.*'
        action: drop

  - job_name: 'cgroup_exporter'
    static_configs:
      - targets: 
        - 'dgx-01:9306'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm

  - job_name: 'nvidia_gpu_exporter'
    static_configs:
      - targets: 
        - 'dgx-01:9445'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm

```

#### Host: statsrv

```bash
# Step 1 - Create Prometheus systemd service

# Create systemd service for Prometheus
cat > /tmp/prometheus.service << 'EOF'
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090 \
    --web.enable-lifecycle

[Install]
WantedBy=multi-user.target
EOF
cp /tmp/prometheus.service /etc/systemd/system/
rm /tmp/prometheus.service

# Step 2 - Start Prometheus service

systemctl daemon-reload && systemctl enable prometheus && systemctl start prometheus

```

---

## 7. Grafana

### Description

This section sets up Grafana for visualizing
the collected metrics and creating dashboards.

### What we'll do

- Install Grafana
- Start and enable Grafana service
- Automatically configure Prometheus data source
- Automatically change default admin password

#### Host: statsrv

```bash
# Step 1 - Add Grafana repository key

wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -

# Step 2 - Add Grafana repository

echo "deb https://packages.grafana.com/oss/deb stable main" > /etc/apt/sources.list.d/grafana.list

# Step 3 - Update package lists

apt update

# Step 4 - Install Grafana

apt install -y grafana

```

---

#### Host: statsrv

```bash
# Step 1 - Start Grafana service

systemctl daemon-reload && systemctl enable grafana-server && systemctl start grafana-server

```

---


### Automated Configuration

#### Host: statsrv

```bash
# Step 1 - Wait for Grafana to start

sleep 15

# Step 2 - Add Prometheus data source

curl -X POST -H "Content-Type: application/json" -d '{"name":"Prometheus","type":"prometheus","url":"http://statsrv:9090","access":"proxy","isDefault":true}' http://admin:admin@localhost:3000/api/datasources

# Step 3 - Change default admin password

curl -X PUT -H "Content-Type: application/json" -d '{"oldPassword":"admin","newPassword":"jobstats123","confirmNew":"jobstats123"}' http://admin:admin@localhost:3000/api/user/password

```

---

## 8. Open OnDemand Jobstats Helper

### Description

This section sets up Open OnDemand integration
for easy access to job statistics.

### What we'll do

- Clone jobstats OOD helper files
- Configure OOD integration
- Test OOD jobstats functionality

#### Host: BCM Headnode

```bash
# Step 1 - Create working directory for OOD helper

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone jobstats repository for OOD helper

cd /opt/jobstats-deployment && git clone https://github.com/PrincetonUniversity/jobstats.git

# Step 3 - List OOD helper files

ls -la /opt/jobstats-deployment/jobstats/ood-jobstats-helper/

```

---

## 9. The jobstats Command

### Description

This section installs the jobstats command-line tool
for querying job statistics from the command line.

### What we'll do

- Install Python dependencies (requests, blessed)
- Install jobstats binary and Python files
- Configure jobstats for your cluster
- Test jobstats command functionality

#### Host: slogin

```bash
# Step 1 - Create /usr/local/jobstats directory

mkdir -p /usr/local/jobstats

# Step 2 - Install jobstats binary

cp /opt/jobstats-deployment/jobstats/jobstats /usr/local/jobstats/

# Step 3 - Install jobstats.py

cp /opt/jobstats-deployment/jobstats/jobstats.py /usr/local/jobstats/

# Step 4 - Install output_formatters.py

cp /opt/jobstats-deployment/jobstats/output_formatters.py /usr/local/jobstats/

# Step 5 - Install config.py

cp /opt/jobstats-deployment/jobstats/config.py /usr/local/jobstats/

# Step 6 - Make jobstats executable

chmod +x /usr/local/jobstats/jobstats

# Step 7 - Create symlink for jobstats command

ln -sf /usr/local/jobstats/jobstats /usr/local/bin/jobstats

```

---

#### Host: slogin

```bash
# Step 1 - Update package list

apt update

# Step 2 - Install Python dependencies (requests, blessed)

apt install -y python3-requests python3-blessed

```

---

#### Host: slogin

```bash
# Step 1 - Update Prometheus server address in config.py

sed -i "s|http://cluster-stats:8480|http://statsrv:9090|g" /usr/local/jobstats/config.py

# Step 2 - Update Prometheus retention days in config.py

sed -i "s|PROM_RETENTION_DAYS = 365|PROM_RETENTION_DAYS = 365|g" /usr/local/jobstats/config.py

```

---

## 10. Additional BCM Configurations

### Description

This section configures BCM category-based service management
and provides instructions for BCM imaging workflow.

### What we'll do

- 10a. Configure BCM category-based service management
- 10b. BCM imaging workflow instructions

### 10a. Configure BCM category-based service management

#### BCM Category Service Management

BCM provides category-based service management where services are
automatically started/stopped when nodes switch between categories.

**Jobstats services should be configured as follows:**
- **Slurm category:** Services enabled with autostart=yes
- **Kubernetes category:** Services disabled with autostart=no

### Commands for Slurm Category Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Add cgroup_exporter to dgx category

cmsh -c "category; use dgx; services; add cgroup_exporter; commit"

# Step 2 - Configure cgroup_exporter for dgx category

cmsh -c "category; use dgx; services; use cgroup_exporter; set autostart yes; set monitored yes; commit"

# Step 3 - Add node_exporter to dgx category

cmsh -c "category; use dgx; services; add node_exporter; commit"

# Step 4 - Configure node_exporter for dgx category

cmsh -c "category; use dgx; services; use node_exporter; set autostart yes; set monitored yes; commit"

# Step 5 - Add nvidia_gpu_prometheus_exporter to dgx category

cmsh -c "category; use dgx; services; add nvidia_gpu_prometheus_exporter; commit"

# Step 6 - Configure nvidia_gpu_prometheus_exporter for dgx category

cmsh -c "category; use dgx; services; use nvidia_gpu_prometheus_exporter; set autostart yes; set monitored yes; commit"

```

---

### Commands for Kubernetes Category Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Add cgroup_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add cgroup_exporter; commit"

# Step 2 - Configure cgroup_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use cgroup_exporter; set autostart no; set monitored no; commit"

# Step 3 - Add node_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add node_exporter; commit"

# Step 4 - Configure node_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use node_exporter; set autostart no; set monitored no; commit"

# Step 5 - Add nvidia_gpu_prometheus_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add nvidia_gpu_prometheus_exporter; commit"

# Step 6 - Configure nvidia_gpu_prometheus_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use nvidia_gpu_prometheus_exporter; set autostart no; set monitored no; commit"

```

---

#### BCM Category Service Configuration

**Slurm Category Configuration:**

```bash
# Configure services for Slurm category (dgx)
cmsh -c "category; use dgx; services; add cgroup_exporter; commit"
cmsh -c "category; use dgx; services; use cgroup_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use dgx; services; add node_exporter; commit"
cmsh -c "category; use dgx; services; use node_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use dgx; services; add nvidia_gpu_prometheus_exporter; commit"
cmsh -c "category; use dgx; services; use nvidia_gpu_prometheus_exporter; set autostart yes; set monitored yes; commit"
```

**Kubernetes Category Configuration:**

```bash
# Configure services for Kubernetes category (runai)
cmsh -c "category; use runai; services; add cgroup_exporter; commit"
cmsh -c "category; use runai; services; use cgroup_exporter; set autostart no; set monitored no; commit"
cmsh -c "category; use runai; services; add node_exporter; commit"
cmsh -c "category; use runai; services; use node_exporter; set autostart no; set monitored no; commit"
cmsh -c "category; use runai; services; add nvidia_gpu_prometheus_exporter; commit"
cmsh -c "category; use runai; services; use nvidia_gpu_prometheus_exporter; set autostart no; set monitored no; commit"
```

### 10b. BCM imaging workflow instructions

#### BCM Imaging Process

After successful deployment on representative nodes, capture images
for each node type to enable deployment to all nodes of the same type.

#### Imaging Commands

Execute the following commands to capture images for each node type:

**Slurm Controller/Login Node (slogin):**

```bash
cmsh -c 'device;use slogin;grabimage -w'
```

**DGX Compute Node (dgx-01):**

```bash
cmsh -c 'device;use dgx-01;grabimage -w'
```

**Monitoring Server (statsrv):**

```bash
cmsh -c 'device;use statsrv;grabimage -w'
```

#### Imaging Benefits

- **Consistent deployment:** All nodes of the same type get identical configuration
- **Fast scaling:** New nodes automatically get the correct configuration
- **Easy maintenance:** Updates can be applied to images and redeployed
- **Service management:** Category-based services automatically start/stop

### 10c. Setcap Commands Notification

#### Important: Setcap Commands and Extended Attributes

During the deployment, the following `setcap` commands were executed:

**dgx-01:**
```bash
setcap cap_sys_ptrace=eip /usr/local/bin/cgroup_exporter
```

**dgx-01:**
```bash
setcap cap_sys_ptrace=eip /usr/local/bin/nvidia_gpu_prometheus_exporter
```

#### Extended Attributes (xattrs) Warning

**IMPORTANT:** If your software images are stored on a file system
that does not support extended attributes (xattrs), such as an NFS export,
you may need to take additional steps to ensure that these `setcap` commands
are run when your systems boot.

**Recommended solutions:**
- Add the `setcap` commands to your system startup scripts
- Include them in your BCM imaging process
- Create a systemd service that runs the commands on boot

---

# ----- NOT INCLUDED IN automation/logs/guided_setup_document.md -----

## BCM Node Imaging Workflow

**IMPORTANT**: This section contains additional steps that should be performed after completing the guided setup process.

### Overview

For BCM-managed systems, the recommended approach is to deploy jobstats on one representative node of each type, then capture and deploy the image using BCM's imaging system.

### Imaging Process

#### Step 1: Deploy on Representative Nodes

Deploy jobstats on one node of each type (DGX, Slurm controller, login node) following the manual deployment steps in this guide.

#### Step 2: Capture Images

After successful deployment, capture the image for each node type:

```bash
# Capture DGX node image
cmsh -c "device;use dgx-node-01;grabimage -w"

# Capture Slurm controller image
cmsh -c "device;use slurm-controller-01;grabimage -w"

# Capture login node image
cmsh -c "device;use login-node-01;grabimage -w"
```

#### Step 3: Deploy to All Nodes

BCM will automatically deploy the captured images to all nodes of the same type.

### Imaging Safety Analysis

**âœ… All jobstats files are safe for BCM imaging:**
- **Shared storage files** (`/cm/shared/apps/slurm/var/cm/`) - meant to be identical
- **Binary files** (`/usr/local/bin/`) - identical across all nodes
- **Systemd services** (`/etc/systemd/system/`) - identical across all nodes
- **Configuration files** - use hostname templating that works on each node
- **No host-specific files** created that would cause conflicts

**No exclude list modifications needed** - all jobstats files are compatible with BCM imaging.

### BCM Category-Based Service Management

Jobstats services are tied to the Slurm category so they automatically start/stop when nodes switch between categories. This ensures:

- **Same software image** used for all DGX nodes
- **Automatic service management** based on category assignment
- **No manual intervention** when switching between Slurm and Kubernetes
- **Clean separation** of services by workload type

#### How BCM Category Service Management Works

BCM provides category-based service management where:
1. **Services are defined at category level** using `cmsh` or Base View
2. **When a node changes category**, BCM automatically:
   - Stops services from the old category (if `autostart` was enabled)
   - Starts services from the new category (if `autostart` is enabled)
3. **No reboot required** - services are managed dynamically
4. **Same image used** - only service configuration changes

#### Implementation Steps

##### Step 1: Create Slurm Category (if not exists)
```bash
# Check existing categories
cmsh -c "category; list"

# Clone default category for Slurm nodes (if needed)
cmsh -c "category; clone default slurm-category; commit"
```

##### Step 2: Add Jobstats Services to Slurm Category
```bash
# Add jobstats services to the Slurm category
cmsh -c "category; use slurm-category; services; add cgroup_exporter"
cmsh -c "category; use slurm-category; services; add node_exporter" 
cmsh -c "category; use slurm-category; services; add nvidia_gpu_prometheus_exporter"

# Configure each service with autostart and monitoring
cmsh -c "category; use slurm-category; services; use cgroup_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use slurm-category; services; use node_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use slurm-category; services; use nvidia_gpu_prometheus_exporter; set autostart yes; set monitored yes; commit"
```

##### Step 3: Assign Nodes to Categories
```bash
# Assign DGX nodes to Slurm category
cmsh -c "device; use dgx-node-01; set category slurm-category; commit"
cmsh -c "device; use dgx-node-02; set category slurm-category; commit"

# Assign DGX nodes to Kubernetes category (when needed)
cmsh -c "device; use dgx-node-01; set category kubernetes-category; commit"
```

### Category-Based Configuration Example

```json
{
  "bcm_category_management": true,
  "slurm_category": "slurm-category",
  "kubernetes_category": "kubernetes-category",
  "systems": {
    "dgx_nodes": ["dgx-node-01", "dgx-node-02", "dgx-node-03"],
    "slurm_dgx_nodes": ["dgx-node-01", "dgx-node-02"],
    "kubernetes_dgx_nodes": ["dgx-node-03"]
  }
}
```

### Workflow Example

#### Initial Setup
```bash
# 1. Deploy jobstats on representative DGX node following manual deployment steps

# 2. Add services to Slurm category
cmsh -c "category; use slurm-category; services; add cgroup_exporter; set autostart yes; set monitored yes; commit"

# 3. Assign nodes to Slurm category
cmsh -c "device; use dgx-node-01; set category slurm-category; commit"
```

#### Switching to Kubernetes
```bash
# 1. Change node category to Kubernetes
cmsh -c "device; use dgx-node-01; set category kubernetes-category; commit"

# 2. Jobstats services automatically stop
# 3. Kubernetes services automatically start
# 4. No reboot required
```

#### Switching Back to Slurm
```bash
# 1. Change node category back to Slurm
cmsh -c "device; use dgx-node-01; set category slurm-category; commit"

# 2. Jobstats services automatically start
# 3. Kubernetes services automatically stop
# 4. No reboot required
```

### Verification Commands
```bash
# Check service status by category
cmsh -c "category; use slurm-category; services; status"

# Check node category assignment
cmsh -c "device; use dgx-node-01; show category"

# Check service status on specific node
cmsh -c "device; use dgx-node-01; services; status"
```

### Benefits

1. **Dynamic Service Management**: Services automatically start/stop with category changes
2. **Same Image**: All DGX nodes use identical software image
3. **No Reboots**: Category changes don't require node reboots
4. **Centralized Control**: All service management through BCM
5. **Clean Separation**: Slurm and Kubernetes services are isolated
6. **Easy Switching**: Simple category assignment changes node behavior
