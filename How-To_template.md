# BCM Jobstats Guided Setup Document

**Generated on:** Tue Sep 30 04:45:11 PM PDT 2025
**Mode:** Dry Run
**Configuration:** automation/configs/config.json

This document provides a complete record of the jobstats deployment process.
It can be used as a reference for what was done or as a manual implementation guide.

## Table of Contents

1. [Setup Overview](#overview)
2. [CPU Job Statistics](#cpu-job-stats)
3. [GPU Job Statistics](#gpu-job-stats)
4. [Node Statistics](#node-stats)
5. [Job Summaries](#job-summaries)
6. [Prometheus](#prometheus)
7. [Grafana](#grafana)
8. [Open OnDemand Jobstats Helper](#ood)
9. [The jobstats Command](#jobstats-command)
10. [Additional BCM Configurations](#bcm-configurations)

---

## 1. Setup Overview

### Description

The jobstats platform provides comprehensive job monitoring for Slurm clusters.
This guided setup will walk you through each step of the deployment process.

### Setup Process Overview

1. Switch to cgroup-based job accounting
2. Setup exporters (cgroup, node, GPU) on compute nodes
3. Setup prolog/epilog scripts on GPU nodes
4. Setup Prometheus server and configuration
5. Setup slurmctld epilog for job summaries
6. Configure Grafana interface
7. Install jobstats command-line tool

### Reference Documentation

- Princeton University: https://princetonuniversity.github.io/jobstats/setup/overview/

## 2. CPU Job Statistics

### Description

Configure Slurm for cgroup-based job accounting and install cgroup exporter

This section configures Slurm settings for cgroup accounting and installs
the cgroup_exporter for collecting cgroup metrics.

### What we'll do

- 2a. Configure slurm.conf settings (outside autogenerated section)
- 2b. Configure cgroup constraints via BCM cmsh
- 2c. Deploy cgroup_exporter

### 2a. Configure slurm.conf settings (outside autogenerated section)

#### Slurm Configuration for Cgroups

We need to configure Slurm to use cgroup-based job accounting.
These settings must be placed outside the AUTOGENERATED SECTION.

### Commands for Slurm Configuration

#### Host: <slurm controller>

```bash
# Step 1 - Backup slurm.conf before modification

cp /cm/shared/apps/slurm/var/etc/slurm/slurm.conf /cm/shared/apps/slurm/var/etc/slurm/slurm.conf.backup

# Step 2 - Update slurm.conf with cgroup settings

cat > /tmp/update_slurm_conf.py << 'EOF'
import re
import sys

# Read the file
with open('/cm/shared/apps/slurm/var/etc/slurm/slurm.conf', 'r') as f:
    content = f.read()

# Settings to update
settings = {
    'JobAcctGatherType': 'jobacct_gather/cgroup',
    'ProctrackType': 'proctrack/cgroup', 
    'TaskPlugin': 'affinity,cgroup'
}

# Find the AUTOGENERATED SECTION
autogen_match = re.search(r'^# BEGIN AUTOGENERATED SECTION.*$', content, re.MULTILINE)
if not autogen_match:
    print('ERROR: AUTOGENERATED SECTION not found')
    sys.exit(1)

autogen_pos = autogen_match.start()

# Process each setting
for key, value in settings.items():
    # Look for existing setting
    pattern = rf'^{re.escape(key)}\s*=.*$'
    match = re.search(pattern, content, re.MULTILINE)
    
    if match:
        # Replace existing setting
        old_line = match.group(0)
        new_line = f'{key}={value}'
        content = content.replace(old_line, new_line)
        print(f'Updated {key}={value}')
    else:
        # Insert new setting before AUTOGENERATED SECTION
        new_line = f'{key}={value}'
        content = content[:autogen_pos] + new_line + '\n' + content[autogen_pos:]
        print(f'Added {key}={value}')

# Write the updated file
with open('/cm/shared/apps/slurm/var/etc/slurm/slurm.conf', 'w') as f:
    f.write(content)

print('slurm.conf updated successfully')
EOF
python3 /tmp/update_slurm_conf.py

```

---

#### BCM Configuration Commands

The following BCM commands configure the slurmctld epilog:

```bash
cmsh -c "wlm;use slurm;set epilogslurmctld /usr/local/sbin/slurmctldepilog.sh;commit"
```

**Note:** BCM automatically manages prolog/epilog settings in slurm.conf.
The scripts we installed will be automatically discovered and executed.

### 2b. Configure cgroup constraints via BCM

#### BCM Cgroup Configuration

We need to configure three BCM settings to enable cgroup support:

1. **ConstrainRAMSpace**: Enable memory constraints for jobs
2. **ConstrainCores**: Enable CPU core constraints for jobs
3. **SelectTypeParameters**: Set to `CR_CPU_Memory` for cgroup resource tracking

### Commands for BCM Cgroup Configuration

#### Host: <slurm controller>

```bash
# Step 1 - Enable RAM space constraints for slurm cluster via BCM

cmsh -c "wlm;use slurm;cgroups;set constrainramspace yes;commit"

# Step 2 - Enable CPU core constraints for slurm cluster via BCM

cmsh -c "wlm;use slurm;cgroups;set constraincores yes;commit"

# Step 3 - Configure select type parameters for cgroup support on slurm cluster via BCM

cmsh -c "wlm;use slurm;set selecttypeparameters CR_CPU_Memory;commit"

```

---

### 2c. Deploy cgroup_exporter

### Commands for Cgroup Exporter Installation

#### Host: <dgx node>

```bash
# Step 1 - Create working directory

mkdir -p /opt/jobstats-deployment

# Step 2 - Install Go compiler

apt update && apt install -y golang-go

# Step 3 - Clone or update cgroup_exporter

cd /opt/jobstats-deployment && if [ -d cgroup_exporter ]; then cd cgroup_exporter && git pull; else git clone https://github.com/plazonic/cgroup_exporter.git; fi

# Step 4 - Build cgroup_exporter

cd /opt/jobstats-deployment/cgroup_exporter && go build

# Step 5 - Stop cgroup_exporter service (if running)

systemctl stop cgroup_exporter || true && sleep 2

# Step 6 - Install cgroup_exporter binary

cp /opt/jobstats-deployment/cgroup_exporter/cgroup_exporter /usr/local/bin/

# Step 7 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 8 - Create cgroup_exporter systemd service

# Create systemd service for cgroup_exporter
cat > /etc/systemd/system/cgroup_exporter.service << 'EOF'
[Unit]
Description=Cgroup Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/cgroup_exporter --web.listen-address=:9306 --config.paths /slurm --collect.fullslurm
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Step 9 - Start cgroup_exporter service

systemctl daemon-reload && systemctl enable cgroup_exporter && systemctl start cgroup_exporter

# Step 10 - Set capabilities for cgroup_exporter to read procfs

setcap cap_sys_ptrace=eip /usr/local/bin/cgroup_exporter

```

---

### Commands for Cgroup Exporter Verification

#### Host: <dgx node>

```bash
# Step 1 - Check cgroup_exporter service status

systemctl is-active cgroup_exporter

# Step 2 - Test cgroup_exporter metrics endpoint

curl -s http://localhost:9306/metrics | head -5

# Step 3 - Verify cgroup_exporter is listening on port 9306

netstat -tlnp | grep :9306 || ss -tlnp | grep :9306

```

---

## 3. GPU Job Statistics

### Description

This section sets up GPU monitoring and the necessary scripts
to track which GPUs are assigned to which jobs.

### What we'll do

- 3a. Deploy nvidia_gpu_prometheus_exporter
- 3b. GPU Job Ownership Helper (prolog/epilog scripts)

### 3a. Deploy nvidia_gpu_prometheus_exporter

### Commands for NVIDIA GPU Exporter Installation

#### Host: <dgx node>

```bash
# Step 1 - Create working directory

mkdir -p /opt/jobstats-deployment

# Step 2 - Install Go compiler

apt update && apt install -y golang-go

# Step 3 - Clone or update NVIDIA GPU exporter

cd /opt/jobstats-deployment && if [ -d nvidia_gpu_prometheus_exporter ]; then cd nvidia_gpu_prometheus_exporter && git pull; else git clone https://github.com/plazonic/nvidia_gpu_prometheus_exporter.git; fi

# Step 4 - Build NVIDIA GPU exporter

cd /opt/jobstats-deployment/nvidia_gpu_prometheus_exporter && go build

# Step 5 - Stop nvidia_gpu_exporter service (if running)

systemctl stop nvidia_gpu_exporter || true && sleep 2

# Step 6 - Install NVIDIA GPU exporter binary

cp /opt/jobstats-deployment/nvidia_gpu_prometheus_exporter/nvidia_gpu_prometheus_exporter /usr/local/bin/

# Step 7 - Create NVIDIA GPU exporter systemd service

# Create systemd service for NVIDIA GPU exporter
cat > /tmp/nvidia_gpu_exporter.service << 'EOF'
[Unit]
Description=NVIDIA GPU Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/nvidia_gpu_prometheus_exporter --web.listen-address=:9445
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
cp /tmp/nvidia_gpu_exporter.service /etc/systemd/system/
rm /tmp/nvidia_gpu_exporter.service

# Step 8 - Start NVIDIA GPU exporter service

systemctl daemon-reload && systemctl enable nvidia_gpu_exporter && systemctl start nvidia_gpu_exporter

# Step 9 - Set capabilities for nvidia_gpu_prometheus_exporter to read procfs

setcap cap_sys_ptrace=eip /usr/local/bin/nvidia_gpu_prometheus_exporter

```

---

### Commands for GPU Exporter Verification

#### Host: <dgx node>

```bash
# Step 1 - Check NVIDIA GPU exporter service status

systemctl is-active nvidia_gpu_exporter

# Step 2 - Test NVIDIA GPU exporter metrics endpoint

curl -s http://localhost:9445/metrics | head -5

# Step 3 - Verify NVIDIA GPU exporter is listening on port 9445

netstat -tlnp | grep :9445 || ss -tlnp | grep :9445

```

---

### 3b. GPU Job Ownership Helper (prolog/epilog scripts)

#### BCM Script Discovery System

BCM uses a generic prolog/epilog system that automatically calls all scripts
in specific directories. We'll install jobstats scripts using BCM's pattern:

- **Script locations:** `/cm/local/apps/slurm/var/prologs/` and `/cm/local/apps/slurm/var/epilogs/`
- **Shared storage:** `/cm/shared/apps/slurm/var/cm/` (accessible to all nodes)
- **Symlink pattern:** Local symlinks point to shared storage scripts
- **Naming convention:** Use `60-` prefix to run after existing BCM scripts

### Commands for Repository Cloning

#### Host: <slurm controller>

```bash
# Step 1 - Create working directory for jobstats deployment

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone or update jobstats repository

cd /opt/jobstats-deployment && if [ -d jobstats ]; then cd jobstats && git pull; else git clone https://github.com/PrincetonUniversity/jobstats.git; fi

```

---

### Commands for BCM Script Installation

#### Host: <slurm controller>

```bash
# Step 1 - Create shared storage directory for jobstats scripts

mkdir -p /cm/shared/apps/slurm/var/cm

# Step 2 - Copy prolog script to shared storage

cp /opt/jobstats-deployment/jobstats/slurm/prolog.d/gpustats_helper.sh /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh

# Step 3 - Make prolog script executable

chmod +x /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh

# Step 4 - Copy epilog script to shared storage

cp /opt/jobstats-deployment/jobstats/slurm/epilog.d/gpustats_helper.sh /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh

# Step 5 - Make epilog script executable

chmod +x /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh

# Step 6 - Create local prolog directory

mkdir -p /cm/local/apps/slurm/var/prologs

# Step 7 - Create local epilog directory

mkdir -p /cm/local/apps/slurm/var/epilogs

# Step 8 - Create prolog symlink (60- prefix for execution order)

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 9 - Create epilog symlink (60- prefix for execution order)

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

# Step 10 - Create prolog/epilog directories

mkdir -p /cm/local/apps/slurm/var/prologs /cm/local/apps/slurm/var/epilogs

# Step 11 - Create prolog symlink

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 12 - Create epilog symlink

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

```

---

#### Host: BCM Headnode

```bash
# Step 1 - Create prolog/epilog directories on BCM headnode

mkdir -p /cm/local/apps/slurm/var/prologs /cm/local/apps/slurm/var/epilogs

# Step 2 - Create prolog symlink on BCM headnode

ln -sf /cm/shared/apps/slurm/var/cm/prolog-jobstats.sh /cm/local/apps/slurm/var/prologs/60-prolog-jobstats.sh

# Step 3 - Create epilog symlink on BCM headnode

ln -sf /cm/shared/apps/slurm/var/cm/epilog-jobstats.sh /cm/local/apps/slurm/var/epilogs/60-epilog-jobstats.sh

```

---

## 4. Node Statistics

### Description

This section installs the Prometheus node_exporter
on all compute nodes to collect system metrics.

### What we'll do

- Clone node_exporter repository
- Build and install node_exporter
- Create systemd service for node_exporter
- Start and enable service
- Verify node_exporter is running and collecting metrics

### Commands for Node Exporter Installation

#### Host: <dgx node>

```bash
# Step 1 - Clone or update node_exporter

cd /opt/jobstats-deployment && if [ -d node_exporter ]; then cd node_exporter && git pull; else git clone https://github.com/prometheus/node_exporter.git; fi

# Step 2 - Build node_exporter

cd /opt/jobstats-deployment/node_exporter && make build

# Step 3 - Stop node_exporter service (if running)

systemctl stop node_exporter || true && sleep 2

# Step 4 - Install node_exporter binary

cp /opt/jobstats-deployment/node_exporter/node_exporter /usr/local/bin/

# Step 5 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 6 - Create node_exporter systemd service

# Create systemd service for node_exporter
cat > /etc/systemd/system/node_exporter.service << 'EOF'
[Unit]
Description=Node Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/node_exporter --web.listen-address=:9100
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Step 7 - Start node_exporter service

systemctl daemon-reload && systemctl enable node_exporter && systemctl start node_exporter

```

---

### Commands for Node Exporter Verification

#### Host: <dgx node>

```bash
# Step 1 - Check node_exporter service status

systemctl is-active node_exporter

# Step 2 - Test node_exporter metrics endpoint

curl -s http://localhost:9100/metrics | head -5

# Step 3 - Verify node_exporter is listening on port 9100

netstat -tlnp | grep :9100 || ss -tlnp | grep :9100

```

---

## 5. Job Summaries

### Description

This section sets up the slurmctld epilog script
to generate and store job summaries in the Slurm database.

### What we'll do

- Install slurmctld epilog script
- Configure BCM epilogslurmctld setting

### Commands for Repository Cloning

#### Host: <slurm controller>

```bash
# Step 1 - Create working directory for jobstats deployment

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone or update jobstats repository

cd /opt/jobstats-deployment && if [ -d jobstats ]; then cd jobstats && git pull; else git clone https://github.com/PrincetonUniversity/jobstats.git; fi

```

---

### Commands for Slurmctld Epilog Installation

#### Host: <slurm controller>

```bash
# Step 1 - Copy slurmctld epilog script for job summaries

cp /opt/jobstats-deployment/jobstats/slurm/slurmctldepilog.sh /usr/local/sbin/

# Step 2 - Make slurmctld epilog script executable

chmod +x /usr/local/sbin/slurmctldepilog.sh

```

---

### Commands for BCM Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Configure BCM epilogslurmctld setting

cmsh -c "wlm;use slurm;set epilogslurmctld /usr/local/sbin/slurmctldepilog.sh;commit"

```

---

## 6. Prometheus

### Description

This section sets up the Prometheus server
to collect and store metrics from all exporters.

### What we'll do

- Download and install Prometheus
- Create Prometheus configuration
- Create systemd service for Prometheus
- Start and enable Prometheus service

#### Host: <monitoring server>

```bash
# Step 1 - Download Prometheus

wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz

# Step 2 - Extract Prometheus

tar xzf prometheus-2.45.0.linux-amd64.tar.gz

# Step 3 - Stop Prometheus service (if running)

systemctl stop prometheus || true && sleep 2

# Step 4 - Install Prometheus binary

cp prometheus-2.45.0.linux-amd64/prometheus /usr/local/bin/

# Step 5 - Create Prometheus directories

mkdir -p /etc/prometheus /var/lib/prometheus

# Step 6 - Create prometheus user

useradd --no-create-home --shell /bin/false prometheus || true

# Step 7 - Set ownership of data directory

chown prometheus:prometheus /var/lib/prometheus

```

---


### Prometheus Configuration

```yaml
global:
  scrape_interval: 30s
  evaluation_interval: 30s
  external_labels:
    monitor: 'jobstats-slurm'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node_exporter'
    static_configs:
      - targets: 
        - '<dgx node>:9100'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm
      - source_labels: [__name__]
        regex: '^go_.*'
        action: drop

  - job_name: 'cgroup_exporter'
    static_configs:
      - targets: 
        - '<dgx node>:9306'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm

  - job_name: 'nvidia_gpu_exporter'
    static_configs:
      - targets: 
        - '<dgx node>:9445'
    metric_relabel_configs:
      - target_label: cluster
        replacement: slurm

```

#### Host: <monitoring server>

```bash
# Step 1 - Create Prometheus systemd service

# Create systemd service for Prometheus
cat > /tmp/prometheus.service << 'EOF'
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090 \
    --web.enable-lifecycle

[Install]
WantedBy=multi-user.target
EOF
cp /tmp/prometheus.service /etc/systemd/system/
rm /tmp/prometheus.service

# Step 2 - Start Prometheus service

systemctl daemon-reload && systemctl enable prometheus && systemctl start prometheus

```

---

## 7. Grafana

### Description

This section sets up Grafana for visualizing
the collected metrics and creating dashboards.

### What we'll do

- Install Grafana
- Start and enable Grafana service
- Automatically configure Prometheus data source
- Automatically change default admin password

#### Host: <monitoring server>

```bash
# Step 1 - Add Grafana repository key

wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -

# Step 2 - Add Grafana repository

echo "deb https://packages.grafana.com/oss/deb stable main" > /etc/apt/sources.list.d/grafana.list

# Step 3 - Update package lists

apt update

# Step 4 - Install Grafana

apt install -y grafana

```

---

#### Host: <monitoring server>

```bash
# Step 1 - Start Grafana service

systemctl daemon-reload && systemctl enable grafana-server && systemctl start grafana-server

```

---


### Automated Configuration

#### Host: <monitoring server>

```bash
# Step 1 - Wait for Grafana to start

sleep 15

# Step 2 - Add Prometheus data source

curl -X POST -H "Content-Type: application/json" -d '{"name":"Prometheus","type":"prometheus","url":"http://<monitoring server>:9090","access":"proxy","isDefault":true}' http://admin:admin@localhost:3000/api/datasources

# Step 3 - Change default admin password

curl -X PUT -H "Content-Type: application/json" -d '{"oldPassword":"admin","newPassword":"jobstats123","confirmNew":"jobstats123"}' http://admin:admin@localhost:3000/api/user/password

```

---

## 8. Open OnDemand Jobstats Helper

### Description

This section sets up Open OnDemand integration
for easy access to job statistics.

### What we'll do

- Clone jobstats OOD helper files
- Configure OOD integration
- Test OOD jobstats functionality

#### Host: BCM Headnode

```bash
# Step 1 - Create working directory for OOD helper

mkdir -p /opt/jobstats-deployment

# Step 2 - Clone jobstats repository for OOD helper

cd /opt/jobstats-deployment && git clone https://github.com/PrincetonUniversity/jobstats.git

# Step 3 - List OOD helper files

ls -la /opt/jobstats-deployment/jobstats/ood-jobstats-helper/

```

---

## 9. The jobstats Command

### Description

This section installs the jobstats command-line tool
for querying job statistics from the command line.

### What we'll do

- Install Python dependencies (requests, blessed)
- Install jobstats binary and Python files
- Configure jobstats for your cluster
- Test jobstats command functionality

#### Host: <slurm controller>

```bash
# Step 1 - Create /usr/local/jobstats directory

mkdir -p /usr/local/jobstats

# Step 2 - Install jobstats binary

cp /opt/jobstats-deployment/jobstats/jobstats /usr/local/jobstats/

# Step 3 - Install jobstats.py

cp /opt/jobstats-deployment/jobstats/jobstats.py /usr/local/jobstats/

# Step 4 - Install output_formatters.py

cp /opt/jobstats-deployment/jobstats/output_formatters.py /usr/local/jobstats/

# Step 5 - Install config.py

cp /opt/jobstats-deployment/jobstats/config.py /usr/local/jobstats/

# Step 6 - Make jobstats executable

chmod +x /usr/local/jobstats/jobstats

# Step 7 - Create symlink for jobstats command

ln -sf /usr/local/jobstats/jobstats /usr/local/bin/jobstats

```

---

#### Host: <slurm controller>

```bash
# Step 1 - Update package list

apt update

# Step 2 - Install Python dependencies (requests, blessed)

apt install -y python3-requests python3-blessed

```

---

#### Host: <slurm controller>

```bash
# Step 1 - Update Prometheus server address in config.py

sed -i "s|http://cluster-stats:8480|http://<monitoring server>:9090|g" /usr/local/jobstats/config.py

# Step 2 - Update Prometheus retention days in config.py

sed -i "s|PROM_RETENTION_DAYS = 365|PROM_RETENTION_DAYS = 365|g" /usr/local/jobstats/config.py

```

---

## 10. Additional BCM Configurations

### Description

This section configures BCM category-based service management
and provides instructions for BCM imaging workflow.

### What we'll do

- 10a. Configure BCM category-based service management
- 10b. BCM imaging workflow instructions

### 10a. Configure BCM category-based service management

#### BCM Category Service Management

BCM provides category-based service management where services are
automatically started/stopped when nodes switch between categories.

**Jobstats services should be configured as follows:**
- **Slurm category:** Services enabled with autostart=yes
- **Kubernetes category:** Services disabled with autostart=no

### Commands for Slurm Category Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Add cgroup_exporter to dgx category

cmsh -c "category; use dgx; services; add cgroup_exporter; commit"

# Step 2 - Configure cgroup_exporter for dgx category

cmsh -c "category; use dgx; services; use cgroup_exporter; set autostart yes; set monitored yes; commit"

# Step 3 - Add node_exporter to dgx category

cmsh -c "category; use dgx; services; add node_exporter; commit"

# Step 4 - Configure node_exporter for dgx category

cmsh -c "category; use dgx; services; use node_exporter; set autostart yes; set monitored yes; commit"

# Step 5 - Add nvidia_gpu_prometheus_exporter to dgx category

cmsh -c "category; use dgx; services; add nvidia_gpu_prometheus_exporter; commit"

# Step 6 - Configure nvidia_gpu_prometheus_exporter for dgx category

cmsh -c "category; use dgx; services; use nvidia_gpu_prometheus_exporter; set autostart yes; set monitored yes; commit"

```

---

### Commands for Kubernetes Category Configuration

#### Host: BCM Headnode

```bash
# Step 1 - Add cgroup_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add cgroup_exporter; commit"

# Step 2 - Configure cgroup_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use cgroup_exporter; set autostart no; set monitored no; commit"

# Step 3 - Add node_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add node_exporter; commit"

# Step 4 - Configure node_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use node_exporter; set autostart no; set monitored no; commit"

# Step 5 - Add nvidia_gpu_prometheus_exporter to runai category (disabled)

cmsh -c "category; use runai; services; add nvidia_gpu_prometheus_exporter; commit"

# Step 6 - Configure nvidia_gpu_prometheus_exporter for runai category (disabled)

cmsh -c "category; use runai; services; use nvidia_gpu_prometheus_exporter; set autostart no; set monitored no; commit"

```

---

#### BCM Category Service Configuration

**Slurm Category Configuration:**

```bash
# Configure services for Slurm category (dgx)
cmsh -c "category; use dgx; services; add cgroup_exporter; commit"
cmsh -c "category; use dgx; services; use cgroup_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use dgx; services; add node_exporter; commit"
cmsh -c "category; use dgx; services; use node_exporter; set autostart yes; set monitored yes; commit"
cmsh -c "category; use dgx; services; add nvidia_gpu_prometheus_exporter; commit"
cmsh -c "category; use dgx; services; use nvidia_gpu_prometheus_exporter; set autostart yes; set monitored yes; commit"
```

**Kubernetes Category Configuration:**

```bash
# Configure services for Kubernetes category (runai)
cmsh -c "category; use runai; services; add cgroup_exporter; commit"
cmsh -c "category; use runai; services; use cgroup_exporter; set autostart no; set monitored no; commit"
cmsh -c "category; use runai; services; add node_exporter; commit"
cmsh -c "category; use runai; services; use node_exporter; set autostart no; set monitored no; commit"
cmsh -c "category; use runai; services; add nvidia_gpu_prometheus_exporter; commit"
cmsh -c "category; use runai; services; use nvidia_gpu_prometheus_exporter; set autostart no; set monitored no; commit"
```

### 10b. BCM imaging workflow instructions

#### BCM Imaging Process

After successful deployment on representative nodes, capture images
for each node type to enable deployment to all nodes of the same type.

#### Imaging Commands

Execute the following commands to capture images for each node type:

**Slurm Controller/Login Node (<slurm controller>):**

```bash
cmsh -c 'device;use <slurm controller>;grabimage -w'
```

**DGX Compute Node (<dgx node>):**

```bash
cmsh -c 'device;use <dgx node>;grabimage -w'
```

**Monitoring Server (<monitoring server>):**

```bash
cmsh -c 'device;use <monitoring server>;grabimage -w'
```

#### Imaging Benefits

- **Consistent deployment:** All nodes of the same type get identical configuration
- **Fast scaling:** New nodes automatically get the correct configuration
- **Easy maintenance:** Updates can be applied to images and redeployed
- **Service management:** Category-based services automatically start/stop

### 10c. Setcap Commands Notification

#### Important: Setcap Commands and Extended Attributes

During the deployment, the following `setcap` commands were executed:

**<dgx node>:**
```bash
setcap cap_sys_ptrace=eip /usr/local/bin/cgroup_exporter
```

**<dgx node>:**
```bash
setcap cap_sys_ptrace=eip /usr/local/bin/nvidia_gpu_prometheus_exporter
```

#### Extended Attributes (xattrs) Warning

**IMPORTANT:** If your software images are stored on a file system
that does not support extended attributes (xattrs), such as an NFS export,
you may need to take additional steps to ensure that these `setcap` commands
are run when your systems boot.

**Recommended solutions:**
- Add the `setcap` commands to your system startup scripts
- Include them in your BCM imaging process
- Create a systemd service that runs the commands on boot

---

# ----- NOT INCLUDED IN automation/logs/guided_setup_document.md -----

## Testing Visual Features and Grafana Dashboards

**IMPORTANT**: This section covers testing the visual and UI-based features of jobstats that are not included in the automated guided setup process.

### Overview

After completing the basic jobstats deployment, you now have access to several powerful visual tools for monitoring and analyzing job performance:

1. **Grafana Dashboards** - Comprehensive visual monitoring
2. **Open OnDemand Integration** - Web-based job statistics access
3. **Additional Monitoring Tools** - GPU dashboard and utilization reports

### Prerequisites for Visual Testing

Before testing the visual features, ensure you have:

- ✅ **Completed the basic jobstats deployment** (all previous sections)
- ✅ **Prometheus server running** and collecting metrics
- ✅ **Grafana server running** with Prometheus data source configured
- ✅ **At least one completed Slurm job** with GPU usage (for testing)
- ✅ **Network access** to Grafana web interface

### 1. Grafana Dashboard Testing

#### 1.1 Import the Jobstats Dashboard

The jobstats repository includes a pre-built Grafana dashboard specifically designed for job monitoring.

**Step 1: Access Grafana**
```bash
# Open Grafana in your web browser
# Default URL: http://<monitoring server>:3000
# Default credentials: admin/admin (or admin/jobstats123 if changed during setup)
```

**Step 2: Import the Dashboard**
1. In Grafana, go to **"+" → "Import"**
2. Click **"Upload JSON file"**
3. Upload the file: `/opt/jobstats-deployment/jobstats/grafana/Single_Job_Stats.json`
4. Click **"Load"**
5. Select your Prometheus data source
6. Click **"Import"**

#### 1.2 Test the Dashboard with a Real Job

**Step 1: Find a Completed Job ID**
```bash
# On <slurm controller>, find a recent completed job
squeue -u $USER --states=CD --format="%.10i %.8u %.2t %.10M %.6D %R" | head -5

# Or check your job history
sacct -u $USER --start=today --format=JobID,JobName,State,ExitCode | head -10
```

**Step 2: Use the Dashboard**
1. Open the **"Single Job Stats"** dashboard in Grafana
2. In the **"Slurm JobID"** field, enter your job ID
3. Click **"Apply"** or press Enter
4. The dashboard will automatically populate with data for that job

#### 1.3 Dashboard Features to Test

The dashboard provides comprehensive visualizations across several categories:

**Job-Level Metrics (CPU & Memory):**
- **CPU Utilization**: User, System, and Total CPU usage over time
- **CPU Memory Utilization**: Total allocated, RSS, Cache, and Used memory

**Job-Level Metrics (GPU):**
- **GPU Utilization**: Percentage utilization for each GPU
- **GPU Memory Utilization**: Memory usage in bytes for each GPU
- **GPU Temperature**: Temperature monitoring for each GPU
- **GPU Power Usage**: Power consumption in milliwatts

**Node-Level Metrics:**
- **CPU Percentage Utilization**: System, User, IO-Wait, and Total
- **Total Memory Utilization**: Total, Used, Available, Buffers, Free, Cached
- **CPU Frequency**: Average frequency across all CPUs with throttling info
- **NFS Statistics**: Read/write requests and metadata operations
- **Local Disk I/O**: Read/write bytes and IOPS
- **GPFS Bandwidth**: File system read/write operations
- **Infiniband Metrics**: Throughput, packet rates, and error monitoring

### 2. Open OnDemand Integration Testing

#### 2.1 Deploy the OOD Helper App

The OOD helper provides a web interface for easy access to job statistics.

**Step 1: Install the OOD Helper**
```bash
# On <monitoring server> or OOD server
# Copy the OOD helper to the OOD apps directory
cp -r /opt/jobstats-deployment/jobstats/ood-jobstats-helper /var/www/ood/apps/sys/jobstats

# Set proper permissions
chown -R apache:apache /var/www/ood/apps/sys/jobstats
chmod -R 755 /var/www/ood/apps/sys/jobstats
```

**Step 2: Configure OOD Integration**
```bash
# Edit the OOD configuration to point to your Grafana instance
# This typically involves updating the OOD configuration files
# to include the Grafana URL and dashboard settings
```

#### 2.2 Test OOD Integration

**Step 1: Access the OOD Interface**
```bash
# Open OOD in your web browser
# URL: http://<ood server>/pun/sys/jobstats
```

**Step 2: Test Job Lookup**
1. Enter a job ID in the web form
2. Select your cluster from the dropdown
3. Click submit
4. Verify that it redirects to the correct Grafana dashboard with the job data

**Step 3: Test Direct URL Access**
```bash
# Test direct URL access (if configured)
# URL: http://<ood server>/pun/sys/jobstats/<cluster>/<jobid>
# Example: http://<ood server>/pun/sys/jobstats/slurm/12345
```

### 3. Additional Monitoring Tools

#### 3.1 GPU Dashboard (gpudash)

The `gpudash` tool provides a text-based real-time view of GPU utilization across the cluster.

**Step 1: Install gpudash**
```bash
# Clone the gpudash repository
cd /opt/jobstats-deployment
git clone https://github.com/PrincetonUniversity/gpudash.git
cd gpudash

# Install Python dependencies
pip3 install blessed requests

# Make the script executable
chmod +x gpudash.py
```

**Step 2: Test gpudash**
```bash
# Run gpudash to see current GPU utilization
python3 gpudash.py

# Run with custom parameters
python3 gpudash.py --columns 5 --interval 5

# Check for available GPUs
python3 gpudash.py --available
```

#### 3.2 Job Defense Shield

Job Defense Shield provides automated monitoring and alerting for underutilized jobs.

**Step 1: Install Job Defense Shield**
```bash
# Clone the repository
cd /opt/jobstats-deployment
git clone https://github.com/PrincetonUniversity/job_defense_shield.git
cd job_defense_shield

# Install dependencies
pip3 install -r requirements.txt
```

**Step 2: Configure and Test**
```bash
# Edit configuration file
cp config.json.example config.json
# Update config.json with your Prometheus server details

# Run a test report
python3 job_defense_shield.py --config config.json --dry-run

# Generate utilization report
python3 job_defense_shield.py --config config.json --report
```

### 4. Verification Checklist

Use this checklist to verify all visual features are working correctly:

#### Grafana Dashboard
- [ ] Dashboard imports successfully without errors
- [ ] Job ID input field accepts valid job IDs
- [ ] CPU utilization graphs display data
- [ ] GPU utilization graphs display data
- [ ] Memory utilization graphs display data
- [ ] Node-level metrics show data
- [ ] Time range selector works correctly
- [ ] All panels load without "No data" errors

#### Open OnDemand Integration
- [ ] OOD helper app loads in web browser
- [ ] Job ID lookup returns correct results
- [ ] Direct URL access works
- [ ] Grafana dashboard opens with correct job data
- [ ] Cluster selection works properly

#### Additional Tools
- [ ] gpudash displays current GPU status
- [ ] Job Defense Shield generates reports
- [ ] All tools can connect to Prometheus
- [ ] Error handling works for invalid inputs

### 5. Troubleshooting Visual Features

#### Common Issues and Solutions

**Dashboard Shows "No Data":**
```bash
# Check if Prometheus is collecting data
curl -s http://<monitoring server>:9090/api/v1/query?query=up

# Verify job ID exists in Prometheus
curl -s "http://<monitoring server>:9090/api/v1/query?query=cgroup_cpus{jobid=\"<jobid>\"}"
```

**Grafana Dashboard Import Fails:**
- Ensure Grafana version is 8.1.2 or compatible
- Check that Prometheus data source is properly configured
- Verify JSON file is not corrupted

**OOD Helper Not Working:**
- Check OOD server logs: `/var/log/ood/passenger.log`
- Verify file permissions on the app directory
- Ensure OOD is configured to allow the jobstats app

**Tools Can't Connect to Prometheus:**
- Verify Prometheus server is running: `systemctl status prometheus`
- Check network connectivity: `telnet <monitoring server> 9090`
- Verify firewall rules allow connections

### 6. Advanced Usage

#### Custom Dashboard Creation
- Create additional Grafana dashboards for specific use cases
- Add custom metrics and visualizations
- Set up automated alerts based on job performance

#### Integration with Other Tools
- Connect jobstats data to external monitoring systems
- Integrate with ticketing systems for automated reporting
- Set up automated email notifications for job performance issues

#### Performance Optimization
- Tune Prometheus retention settings for your needs
- Optimize Grafana dashboard queries for better performance
- Set up data archiving for long-term storage

### 7. Next Steps

After successfully testing the visual features:

1. **Train Users**: Provide documentation on how to use the Grafana dashboards
2. **Set Up Monitoring**: Configure alerts for system health and job performance
3. **Customize Dashboards**: Modify dashboards for your specific cluster needs
4. **Integrate with Workflows**: Incorporate jobstats into your existing monitoring workflows
5. **Scale Testing**: Test with multiple concurrent jobs and high cluster utilization

The visual features of jobstats provide powerful insights into job performance and system utilization, making it an invaluable tool for both users and administrators of HPC clusters.
